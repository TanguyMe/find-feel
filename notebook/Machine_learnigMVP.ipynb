{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import Pandas\n",
    "import pandas as pd\n",
    "\n",
    "#Set Relative Path\n",
    "import sys\n",
    "sys.path.append(\"../src\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>emotion</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>i didnt feel humiliated</td>\n",
       "      <td>sadness</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>i can go from feeling so hopeless to so damned...</td>\n",
       "      <td>sadness</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>im grabbing a minute to post i feel greedy wrong</td>\n",
       "      <td>anger</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>i am ever feeling nostalgic about the fireplac...</td>\n",
       "      <td>love</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>i am feeling grouchy</td>\n",
       "      <td>anger</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  emotion\n",
       "0                            i didnt feel humiliated  sadness\n",
       "1  i can go from feeling so hopeless to so damned...  sadness\n",
       "2   im grabbing a minute to post i feel greedy wrong    anger\n",
       "3  i am ever feeling nostalgic about the fireplac...     love\n",
       "4                               i am feeling grouchy    anger"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Import data\n",
    "df_dataK = pd.read_csv(r'../data/d03_cleaned_data/CleanKaggle.csv')\n",
    "df_dataK.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/apprenant/anaconda3/lib/python3.8/site-packages/sklearn/feature_extraction/text.py:383: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['amp', 'aren', 'couldn', 'didn', 'doesn', 'don', 'hadn', 'hasn', 'haven', 'isn', 'let', 'll', 'mustn', 're', 'shan', 'shouldn', 've', 'wasn', 'weren', 'won', 'wouldn'] not in stop_words.\n",
      "  warnings.warn('Your stop_words may be inconsistent with '\n",
      "[4 0 3 5 1 2]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer \n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from wordcloud import STOPWORDS\n",
    "STOPWORDS.update(['feel','feeling','im',\",\",\"t\",\"u\",\"2\",\"'\",\"&amp;\",\"-\",\"...\",\"s\"])\n",
    "\n",
    "# Set X and y\n",
    "tf = TfidfVectorizer(analyzer='word', ngram_range=(1, 2), min_df=0, stop_words=STOPWORDS)\n",
    "tfidf_matrix = tf.fit_transform(df_dataK['text'])\n",
    "X = tfidf_matrix\n",
    "\n",
    "Labelenc = LabelEncoder()\n",
    "df_dataK['emotion'] = Labelenc.fit_transform(df_dataK['emotion'])\n",
    "y = df_dataK['emotion']\n",
    "print (y.unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "i love you so much!!!!\n"
     ]
    }
   ],
   "source": [
    "# Prediction\n",
    "#userinput= str(input()) \n",
    "userinput = \"i love you so much!!!!\"\n",
    "print (userinput)\n",
    "x = tf.transform([userinput])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/apprenant/anaconda3/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:762: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "LogisticRegression()\n",
      "Train R2 = 0.881; Train RMSE = 0.504; Test accuracy = 0.953\n",
      "Test R2  = 0.454; Test RMSE = 1.096; Test accuracy = 0.781\n",
      "[2]\n",
      "['happy']\n",
      "\n",
      "\n",
      "SGDClassifier()\n",
      "Train R2 = 0.967; Train RMSE = 0.267; Test accuracy = 0.989\n",
      "Test R2  = 0.675; Test RMSE = 0.846; Test accuracy = 0.883\n",
      "[2]\n",
      "['happy']\n",
      "\n",
      "\n",
      "RandomForestClassifier()\n",
      "Train R2 = 0.992; Train RMSE = 0.129; Test accuracy = 0.997\n",
      "Test R2  = 0.671; Test RMSE = 0.844; Test accuracy = 0.886\n",
      "[2]\n",
      "['happy']\n",
      "\n",
      "\n",
      "/home/apprenant/anaconda3/lib/python3.8/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n",
      "[10:40:03] WARNING: ../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
      "              colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,\n",
      "              importance_type='gain', interaction_constraints='',\n",
      "              learning_rate=0.300000012, max_delta_step=0, max_depth=6,\n",
      "              min_child_weight=1, missing=nan, monotone_constraints='()',\n",
      "              n_estimators=100, n_jobs=8, num_parallel_tree=1,\n",
      "              objective='multi:softprob', random_state=0, reg_alpha=0,\n",
      "              reg_lambda=1, scale_pos_weight=None, subsample=1,\n",
      "              tree_method='exact', validate_parameters=1, verbosity=None)\n",
      "Train R2 = 0.819; Train RMSE = 0.626; Test accuracy = 0.931\n",
      "Test R2  = 0.674; Test RMSE = 0.824; Test accuracy = 0.877\n",
      "[2]\n",
      "['happy']\n",
      "\n",
      "\n",
      "Epoch 1/10\n",
      "172/172 [==============================] - 5s 23ms/step - loss: -2760.4755 - accuracy: 0.1204\n",
      "Epoch 2/10\n",
      "172/172 [==============================] - 4s 23ms/step - loss: -341250.8804 - accuracy: 0.1224\n",
      "Epoch 3/10\n",
      "172/172 [==============================] - 4s 23ms/step - loss: -4005081.4451 - accuracy: 0.1201\n",
      "Epoch 4/10\n",
      "172/172 [==============================] - 4s 23ms/step - loss: -18915140.8960 - accuracy: 0.1231\n",
      "Epoch 5/10\n",
      "172/172 [==============================] - 4s 23ms/step - loss: -55409853.5954 - accuracy: 0.1222\n",
      "Epoch 6/10\n",
      "172/172 [==============================] - 4s 23ms/step - loss: -126655068.9480 - accuracy: 0.1194\n",
      "Epoch 7/10\n",
      "172/172 [==============================] - 4s 23ms/step - loss: -248023889.4798 - accuracy: 0.1222\n",
      "Epoch 8/10\n",
      "172/172 [==============================] - 4s 23ms/step - loss: -437109604.8092 - accuracy: 0.1176\n",
      "Epoch 9/10\n",
      "172/172 [==============================] - 4s 23ms/step - loss: -709360236.7630 - accuracy: 0.1246\n",
      "Epoch 10/10\n",
      "172/172 [==============================] - 4s 24ms/step - loss: -1054054253.5029 - accuracy: 0.1221\n",
      "/home/apprenant/anaconda3/lib/python3.8/site-packages/keras/engine/sequential.py:450: UserWarning: `model.predict_classes()` is deprecated and will be removed after 2021-01-01. Please use instead:* `np.argmax(model.predict(x), axis=-1)`,   if your model does multi-class classification   (e.g. if it uses a `softmax` last-layer activation).* `(model.predict(x) > 0.5).astype(\"int32\")`,   if your model does binary classification   (e.g. if it uses a `sigmoid` last-layer activation).\n",
      "  warnings.warn('`model.predict_classes()` is deprecated and '\n",
      "<keras.engine.sequential.Sequential object at 0x7fd2dca8bc70>\n",
      " Train accuracy = 0.123\n",
      " Test accuracy = 0.126\n",
      "[[1.]]\n",
      "['fear']\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from reg_lin import get_metrix\n",
    "from sklearn.linear_model import LogisticRegression ,SGDClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from xgboost import XGBClassifier\n",
    "from keras.models import Sequential \n",
    "from tensorflow.keras import layers, preprocessing\n",
    "from tensorflow import keras\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score,accuracy_score\n",
    "models = [  LogisticRegression(),\n",
    "            SGDClassifier(),\n",
    "            RandomForestClassifier(),\n",
    "            XGBClassifier(),\n",
    "            Sequential()]\n",
    "place = 4\n",
    "savedmodel=[]\n",
    "count = 0\n",
    "for modelset in models :\n",
    "    if count == place:\n",
    "        \n",
    "        df_dataKKeras = df_dataK[['text']].copy()\n",
    "        tk = preprocessing.text.Tokenizer()\n",
    "        tk.fit_on_texts(df_dataKKeras['text'])\n",
    "        X = tk.texts_to_matrix(df_dataKKeras['text'], mode='tfidf')\n",
    "        x = tk.texts_to_matrix([userinput], mode='tfidf')\n",
    "      \n",
    "        X_train, X_test, y_train, y_test = train_test_split(X, y, train_size=0.8, random_state=None)\n",
    "        model = modelset\n",
    "        model.add(layers.Dense(160, activation = 'relu', input_shape= [X_train.shape[1]]))\n",
    "        model.add(layers.Dense(80, activation = 'relu'))\n",
    "        model.add(layers.Dense(40, activation='relu'))\n",
    "        model.add(layers.Dense(1,activation='softmax'))\n",
    "        model.compile(optimizer=\"adam\",loss=\"binary_crossentropy\",metrics=['accuracy']) \n",
    "        model.fit(X_train,y_train,epochs=10, batch_size=100)\n",
    "\n",
    "        y_train_pred = model.predict_classes(X_train)\n",
    "        y_test_pred = model.predict_classes(X_test)\n",
    "        \n",
    "        print (model)\n",
    "\n",
    "        print (\" Train accuracy = {}\" \n",
    "        .format(round(accuracy_score(y_train, y_train_pred),3)))\n",
    "\n",
    "        print (\" Test accuracy = {}\" \n",
    "        .format(round(accuracy_score(y_test, y_test_pred),3)))\n",
    "    if count != place:\n",
    "        model = get_metrix(y,X,modelset)\n",
    "    pred = model.predict(x)\n",
    "    print (pred)\n",
    "    if count == place:\n",
    "        pred = Labelenc.inverse_transform([round(pred[0][0])])\n",
    "    if count != place:\n",
    "        pred = Labelenc.inverse_transform(pred)\n",
    "    print (pred)\n",
    "    savedmodel.append(model)\n",
    "    count += 1\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "metadata": {
   "interpreter": {
    "hash": "5b9c502b618e97131917a2f1409b4700bb639cdf99ce16cd88a0e27a90524386"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
