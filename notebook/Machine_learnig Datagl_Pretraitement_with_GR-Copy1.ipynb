{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import Pandas\n",
    "import pandas as pd\n",
    "\n",
    "#Set Relative Path\n",
    "import sys\n",
    "sys.path.append(\"../src\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>emotion</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>i got up feeling horny this morning</td>\n",
       "      <td>love</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>i will go to my mailbox and talk to the mailma...</td>\n",
       "      <td>happy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>i understand that any of my extremely positive...</td>\n",
       "      <td>happy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>i suggest you give it a listen i feel like i a...</td>\n",
       "      <td>love</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>i have been with petronas for years i feel tha...</td>\n",
       "      <td>happy</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text emotion\n",
       "0                i got up feeling horny this morning    love\n",
       "1  i will go to my mailbox and talk to the mailma...   happy\n",
       "2  i understand that any of my extremely positive...   happy\n",
       "3  i suggest you give it a listen i feel like i a...    love\n",
       "4  i have been with petronas for years i feel tha...   happy"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Import data\n",
    "df_dataK = pd.read_csv(r'../data/d03_cleaned_data/datall.csv')\n",
    "df_dataK.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import WordNetLemmatizer\n",
    "def lemmatizer(text):\n",
    "    \"\"\"Apply Wordnet lemmatizer to text (go to root word)\"\"\"\n",
    "    wnl = WordNetLemmatizer()\n",
    "    text = [wnl.lemmatize(word) for word in text.split()]\n",
    "    return \" \".join(text)\n",
    "df_dataK['text'] = df_dataK['text'].apply(lemmatizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# suppression des caratères spéciaux, formatage des contractions...\n",
    "from nltk.corpus import stopwords\n",
    "from emoji import demojize\n",
    "import re\n",
    "\n",
    "def clean_str(texts):\n",
    "    from nltk.corpus import stopwords\n",
    "    # Lowercasing\n",
    "    texts = texts.str.lower()\n",
    "\n",
    "    # Remove special chars\n",
    "    texts = texts.str.replace(r\"(http|@)\\S+\", \"\")\n",
    "    texts = texts.apply(demojize)\n",
    "    texts = texts.str.replace(r\"::\", \": :\")\n",
    "    texts = texts.str.replace(r\"’\", \"'\")\n",
    "    texts = texts.str.replace(r\"[^a-z\\':_]\", \" \")\n",
    "\n",
    "    # Remove repetitions\n",
    "    pattern = re.compile(r\"(.)\\1{2,}\", re.DOTALL)\n",
    "    texts = texts.str.replace(pattern, r\"\\1\")\n",
    "\n",
    "    # Transform short negation form\n",
    "    texts = texts.str.replace(r\"(can't|cannot)\", 'can not')\n",
    "    texts = texts.str.replace(r\"(ain't|wasn't|weren't)\", 'be not')\n",
    "    texts = texts.str.replace(r\"(don't|didn't|didnt)\", 'do not')\n",
    "    texts = texts.str.replace(r\"(haven't|hasn't)\", 'have not')\n",
    "    texts = texts.str.replace(r\"(won't)\", 'will not')\n",
    "    texts = texts.str.replace(r\"(im)\", ' i am')\n",
    "    texts = texts.str.replace(r\"(ive)\", ' i have')\n",
    "    texts = texts.str.replace(r\"(n't)\", ' not')\n",
    "\n",
    "    # Remove stop words\n",
    "    stopwords = stopwords.words('english')\n",
    "    stopwords.remove('not')\n",
    "    stopwords.remove('nor')\n",
    "    stopwords.remove('no')\n",
    "    texts = texts.apply(lambda x: ' '.join([word for word in x.split() if (word not in stopwords and len(word) > 1 )]))\n",
    "    return texts\n",
    "\n",
    "df_dataK['text'] = clean_str(df_dataK['text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from sentence_transformers import SentenceTransformer\n",
    "def model_used(df, model):\n",
    "    \"\"\"Given a model choice, return the model and the computed matrix\"\"\"\n",
    "    if model == 'Tfidf':\n",
    "        tf = TfidfVectorizer(analyzer='word', ngram_range=(1, 2), min_df=0)\n",
    "        tfidf_matrix = tf.fit_transform(df['text'])\n",
    "        return tf, tfidf_matrix\n",
    "    elif model == 'CountVectorizer':\n",
    "        cv = CountVectorizer(analyzer='word', ngram_range=(1, 2), min_df=0)\n",
    "        matrix = cv.fit_transform(df['text'])\n",
    "        return cv, matrix\n",
    "    elif model == 'BERT':\n",
    "#         bert = SentenceTransformer('distiluse-base-multilingual-cased-v1') # Multilingue\n",
    "#         bert = SentenceTransformer('average_word_embeddings_glove.6B.300d') # + rapide\n",
    "        bert = SentenceTransformer('paraphrase-MiniLM-L6-v2') # Meilleur score en théorie, à vérifier sur nos données\n",
    "        matrix = bert.encode(df['text'].astype('str'), show_progress_bar=True)\n",
    "        return bert, matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You try to use a model that was created with version 1.2.0, however, your version is 1.1.0. This might cause unexpected behavior or errors. In that case, try to update to the latest version.\n",
      "\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1c5f9398058c43dbbbc3324cec305f50",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value='Batches'), FloatProgress(value=0.0, max=1489.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer \n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "# Set X and y\n",
    "enc, X = model_used(df_dataK,'BERT')\n",
    "\n",
    "Labelenc = LabelEncoder()\n",
    "df_dataK['emotion'] = Labelenc.fit_transform(df_dataK['emotion'])\n",
    "y = df_dataK['emotion']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from reg_lin import get_metrix\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import RandomizedSearchCV\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/apprenant/anaconda3/lib/python3.8/site-packages/sklearn/linear_model/_sag.py:329: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\"The max_iter was reached which means \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train R2 = -0.29; Train RMSE 1.589\n",
      "Test R2  = -0.35; Test RMSE 1.61\n",
      "Train R2 = -0.417; Train RMSE 1.665\n",
      "Test R2  = -0.46; Test RMSE 1.681\n",
      "Train R2 = 0.981; Train RMSE 0.191\n",
      "Test R2  = -0.33; Test RMSE 1.614\n"
     ]
    }
   ],
   "source": [
    "from reg_lin import get_metrix, get_metrix_v2\n",
    "from sklearn.linear_model import LogisticRegression ,SGDClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from xgboost import XGBClassifier\n",
    "from keras.models import Sequential \n",
    "from tensorflow.keras import layers, preprocessing\n",
    "from tensorflow import keras\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score,accuracy_score\n",
    "\n",
    "\n",
    "models = [  LogisticRegression(n_jobs=-1,multi_class='multinomial',penalty='l2',solver = 'saga' , max_iter=500,C=1 ),\n",
    "            SGDClassifier(n_jobs=-1,penalty='l2', max_iter=500,loss='hinge',alpha=0.001),\n",
    "            RandomForestClassifier(n_jobs=-1,n_estimators= 1000,min_samples_split= 8,min_samples_leaf= 2,criterion= 'entropy')]\n",
    "#             XGBClassifier(),\n",
    "#             Sequential()]\n",
    "\n",
    "place = 4\n",
    "savedmodel=[]\n",
    "count = 0\n",
    "results= {}\n",
    "for modelset in models:\n",
    "    if 'Sequential' in str(modelset):\n",
    "        df_dataKKeras = df_dataK.sample(15000)\n",
    "        y = df_dataKKeras['emotion']\n",
    "\n",
    "        df_dataKKeras = df_dataKKeras[['text']]\n",
    "        tk = preprocessing.text.Tokenizer()\n",
    "        tk.fit_on_texts(df_dataKKeras['text'])\n",
    "        X = tk.texts_to_matrix(df_dataKKeras['text'], mode='tfidf')\n",
    "\n",
    "        X_train, X_test, y_train, y_test = train_test_split(X, y, train_size=0.8, random_state=None)\n",
    "        model = modelset\n",
    "        model.add(layers.Dense(160, activation='relu', input_shape=[X_train.shape[1]]))\n",
    "        model.add(layers.Dense(80, activation='relu'))\n",
    "        model.add(layers.Dense(40, activation='relu'))\n",
    "        model.add(layers.Dense(6, activation='softmax'))\n",
    "        model.compile(optimizer=\"adam\", loss=\"binary_crossentropy\", metrics=['accuracy'])\n",
    "        model.fit(X_train, y_train, epochs=10, batch_size=100)\n",
    "\n",
    "        y_train_pred = model.predict_classes(X_train)\n",
    "        y_test_pred = model.predict_classes(X_test)\n",
    "        y_test_proba = model.predict_proba(X_test)\n",
    "        y_test_pred = Labelenc.inverse_transform([round(y_test_pred[0][0])])\n",
    "        print(\" Train accuracy = {}\"\n",
    "              .format(round(accuracy_score(y_train, y_train_pred), 3)))\n",
    "\n",
    "        print(\" Test accuracy = {}\"\n",
    "              .format(round(accuracy_score(y_test, y_test_pred), 3)))\n",
    "        results['NeuralNetwork'] = [model, y_test_proba, y_test_pred, y_test]\n",
    "    else:\n",
    "        try:\n",
    "            model, y_test_proba, y_test_pred, y_test = get_metrix_v2(y, X, modelset)\n",
    "        except:\n",
    "            model, y_test_pred, y_test = get_metrix(y, X, modelset)\n",
    "            y_test_proba = None\n",
    "        y_test_pred = Labelenc.inverse_transform(y_test_pred)\n",
    "        if 'Logistic' in str(modelset):\n",
    "            results['LogisticRegression'] = [model, y_test_proba, y_test_pred, y_test]\n",
    "        elif 'SGD' in str(modelset):\n",
    "            results['SGDClassifier'] = [model, y_test_proba, y_test_pred, y_test]\n",
    "        elif 'Forest' in str(modelset):\n",
    "            results['RandomForest'] = [model, y_test_proba, y_test_pred, y_test]\n",
    "        elif 'XGB' in str(modelset):\n",
    "            results['Xgboost'] = [model, y_test_proba, y_test_pred, y_test]\n",
    "        else:\n",
    "            results[str(model)] = [model, y_test_proba, y_test_pred, y_test]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'LogisticRegression': [LogisticRegression(C=1, max_iter=500, multi_class='multinomial', n_jobs=-1,\n",
       "                     solver='saga'),\n",
       "  array([[2.79707145e-02, 1.59962565e-01, 2.47402722e-03, 1.89044874e-03,\n",
       "          8.07208240e-01, 4.94063890e-04],\n",
       "         [1.17768683e-02, 3.54708910e-01, 8.21517184e-02, 1.85649972e-02,\n",
       "          4.63497519e-01, 6.92999586e-02],\n",
       "         [1.83975045e-02, 1.06561504e-01, 6.05407774e-01, 8.82487670e-02,\n",
       "          1.00317955e-01, 8.10665861e-02],\n",
       "         ...,\n",
       "         [6.81919104e-04, 8.12693238e-02, 7.08899796e-01, 1.47259817e-01,\n",
       "          4.89583649e-02, 1.29307508e-02],\n",
       "         [1.23182163e-02, 6.89486265e-02, 2.27198750e-01, 6.56326354e-01,\n",
       "          1.75954625e-02, 1.76126696e-02],\n",
       "         [7.23050356e-01, 3.35451402e-02, 8.10149033e-03, 9.34884045e-03,\n",
       "          2.19858065e-01, 6.09621825e-03]], dtype=float32),\n",
       "  array(['sadness', 'sadness', 'happy', ..., 'happy', 'love', 'anger'],\n",
       "        dtype=object),\n",
       "  10936    1\n",
       "  30676    4\n",
       "  36012    1\n",
       "  44996    2\n",
       "  23342    1\n",
       "          ..\n",
       "  32231    4\n",
       "  20838    4\n",
       "  45385    2\n",
       "  1360     1\n",
       "  19441    0\n",
       "  Name: emotion, Length: 9526, dtype: int64],\n",
       " 'SGDClassifier': [SGDClassifier(alpha=0.001, max_iter=500, n_jobs=-1),\n",
       "  None,\n",
       "  array(['happy', 'happy', 'sadness', ..., 'happy', 'happy', 'anger'],\n",
       "        dtype=object),\n",
       "  11054    3\n",
       "  33432    2\n",
       "  3563     4\n",
       "  1183     2\n",
       "  1884     4\n",
       "          ..\n",
       "  8796     2\n",
       "  31955    1\n",
       "  12745    4\n",
       "  44682    2\n",
       "  15661    1\n",
       "  Name: emotion, Length: 9526, dtype: int64],\n",
       " 'RandomForest': [RandomForestClassifier(criterion='entropy', min_samples_leaf=2,\n",
       "                         min_samples_split=8, n_estimators=1000, n_jobs=-1),\n",
       "  array([[0.04169398, 0.14711194, 0.51337929, 0.11316504, 0.10704395,\n",
       "          0.07760579],\n",
       "         [0.03544899, 0.23556206, 0.35172026, 0.14632224, 0.16932527,\n",
       "          0.06162118],\n",
       "         [0.05847385, 0.11306005, 0.49437453, 0.16715774, 0.11490387,\n",
       "          0.05202997],\n",
       "         ...,\n",
       "         [0.04343206, 0.25699807, 0.35647539, 0.10927793, 0.17990629,\n",
       "          0.05391026],\n",
       "         [0.02695285, 0.1834832 , 0.45522165, 0.12445093, 0.12186262,\n",
       "          0.08802875],\n",
       "         [0.06900415, 0.29523952, 0.29580842, 0.0807735 , 0.17717403,\n",
       "          0.08200037]]),\n",
       "  array(['happy', 'happy', 'happy', ..., 'happy', 'happy', 'happy'],\n",
       "        dtype=object),\n",
       "  15585    2\n",
       "  42829    3\n",
       "  912      2\n",
       "  16067    0\n",
       "  14171    4\n",
       "          ..\n",
       "  33481    1\n",
       "  1223     2\n",
       "  33816    4\n",
       "  36496    2\n",
       "  30638    1\n",
       "  Name: emotion, Length: 9526, dtype: int64]}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import joblib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['../dump/results_multi.joblib']"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "joblib.dump(results,\"../dump/results_multi.joblib\", compress=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "inverser = Labelenc.inverse_transform([0,1,2,3,4,5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['anger', 'fear', 'happy', 'love', 'sadness', 'surprise'],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inverser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random as rd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoded = [rd.randint(0,5) for x in range(50)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "revencoded = [inverser[x] for x in encoded]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['anger',\n",
       " 'sadness',\n",
       " 'love',\n",
       " 'fear',\n",
       " 'love',\n",
       " 'happy',\n",
       " 'anger',\n",
       " 'sadness',\n",
       " 'love',\n",
       " 'love',\n",
       " 'love',\n",
       " 'love',\n",
       " 'surprise',\n",
       " 'happy',\n",
       " 'anger',\n",
       " 'anger',\n",
       " 'happy',\n",
       " 'surprise',\n",
       " 'happy',\n",
       " 'happy',\n",
       " 'love',\n",
       " 'happy',\n",
       " 'fear',\n",
       " 'happy',\n",
       " 'anger',\n",
       " 'fear',\n",
       " 'fear',\n",
       " 'happy',\n",
       " 'surprise',\n",
       " 'love',\n",
       " 'surprise',\n",
       " 'anger',\n",
       " 'happy',\n",
       " 'love',\n",
       " 'love',\n",
       " 'sadness',\n",
       " 'surprise',\n",
       " 'surprise',\n",
       " 'sadness',\n",
       " 'love',\n",
       " 'happy',\n",
       " 'love',\n",
       " 'sadness',\n",
       " 'happy',\n",
       " 'love',\n",
       " 'happy',\n",
       " 'sadness',\n",
       " 'anger',\n",
       " 'sadness',\n",
       " 'anger']"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "revencoded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del x,X,y,X_test,X_train,y_test,y_train,y_test_pred,y_train_pred,df_dataK,df_dataKKeras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import data\n",
    "df_dataK = pd.read_csv(r'../data/d03_cleaned_data/datall_test.csv')\n",
    "\n",
    "df_dataK['text'] = df_dataK['text'].apply(lemmatizer)\n",
    "df_dataK['text'] = clean_str(df_dataK['text'])\n",
    "X_test = enc.encode(df_dataK['text'])\n",
    "y_test = Labelenc.transform(df_dataK['emotion'])\n",
    "\n",
    "count = 0\n",
    "place = 4 \n",
    "\n",
    "for model in savedmodel:\n",
    "\n",
    "    print (model)\n",
    "    if count != place:\n",
    "        y_test_pred = model.predict(X_test)\n",
    "    if count == place:\n",
    "        X_test = tk.texts_to_matrix(df_dataK['text'], mode='tfidf')\n",
    "        y_test_pred = model.predict(X_test)\n",
    "    print (\" Test accuracy = {}\" \n",
    "        .format(round(accuracy_score(y_test, y_test_pred),3)))\n",
    "\n",
    "    count += 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "metadata": {
   "interpreter": {
    "hash": "5b9c502b618e97131917a2f1409b4700bb639cdf99ce16cd88a0e27a90524386"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
