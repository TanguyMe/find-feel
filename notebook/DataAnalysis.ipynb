{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.insert(0, '/home/apprenant/PycharmProjects/TP--Find_the_Feel')\n",
    "sys.path.append(\"../src\")\n",
    "# On importe les librairies dont on aura besoin pour ce tp\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from wordcloud import WordCloud, STOPWORDS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from Machine_learning_advanced import model_results_multi, df_dataK_ready"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['text', 'emotion']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(df_dataK_ready.columns.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import joblib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "resultsjb = joblib.load(\"../dump/results_multi.joblib\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "classes=['anger', 'fear', 'happy', 'love', 'sadness', 'surprise']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LogisticRegression\n",
      "0.5275036741549444\n",
      "SGDClassifier\n",
      "0.48687801805584713\n",
      "RandomForest\n",
      "0.47785009447827\n"
     ]
    }
   ],
   "source": [
    "for key, values in resultsjb.items():\n",
    "#     print(values[2])\n",
    "    print(key)\n",
    "    y_true = values[3]\n",
    "    y_true = [classes[x] for x in y_true]\n",
    "#     print(y_true)\n",
    "#     print(values[2])\n",
    "    print(accuracy_score(y_true, values[2]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'LogisticRegression': [LogisticRegression(C=1, max_iter=500, multi_class='multinomial', n_jobs=-1,\n",
       "                     solver='saga'),\n",
       "  array([[2.79707145e-02, 1.59962565e-01, 2.47402722e-03, 1.89044874e-03,\n",
       "          8.07208240e-01, 4.94063890e-04],\n",
       "         [1.17768683e-02, 3.54708910e-01, 8.21517184e-02, 1.85649972e-02,\n",
       "          4.63497519e-01, 6.92999586e-02],\n",
       "         [1.83975045e-02, 1.06561504e-01, 6.05407774e-01, 8.82487670e-02,\n",
       "          1.00317955e-01, 8.10665861e-02],\n",
       "         ...,\n",
       "         [6.81919104e-04, 8.12693238e-02, 7.08899796e-01, 1.47259817e-01,\n",
       "          4.89583649e-02, 1.29307508e-02],\n",
       "         [1.23182163e-02, 6.89486265e-02, 2.27198750e-01, 6.56326354e-01,\n",
       "          1.75954625e-02, 1.76126696e-02],\n",
       "         [7.23050356e-01, 3.35451402e-02, 8.10149033e-03, 9.34884045e-03,\n",
       "          2.19858065e-01, 6.09621825e-03]], dtype=float32),\n",
       "  array(['sadness', 'sadness', 'happy', ..., 'happy', 'love', 'anger'],\n",
       "        dtype=object),\n",
       "  10936    1\n",
       "  30676    4\n",
       "  36012    1\n",
       "  44996    2\n",
       "  23342    1\n",
       "          ..\n",
       "  32231    4\n",
       "  20838    4\n",
       "  45385    2\n",
       "  1360     1\n",
       "  19441    0\n",
       "  Name: emotion, Length: 9526, dtype: int64],\n",
       " 'SGDClassifier': [SGDClassifier(alpha=0.001, max_iter=500, n_jobs=-1),\n",
       "  None,\n",
       "  array(['happy', 'happy', 'sadness', ..., 'happy', 'happy', 'anger'],\n",
       "        dtype=object),\n",
       "  11054    3\n",
       "  33432    2\n",
       "  3563     4\n",
       "  1183     2\n",
       "  1884     4\n",
       "          ..\n",
       "  8796     2\n",
       "  31955    1\n",
       "  12745    4\n",
       "  44682    2\n",
       "  15661    1\n",
       "  Name: emotion, Length: 9526, dtype: int64],\n",
       " 'RandomForest': [RandomForestClassifier(criterion='entropy', min_samples_leaf=2,\n",
       "                         min_samples_split=8, n_estimators=1000, n_jobs=-1),\n",
       "  array([[0.04169398, 0.14711194, 0.51337929, 0.11316504, 0.10704395,\n",
       "          0.07760579],\n",
       "         [0.03544899, 0.23556206, 0.35172026, 0.14632224, 0.16932527,\n",
       "          0.06162118],\n",
       "         [0.05847385, 0.11306005, 0.49437453, 0.16715774, 0.11490387,\n",
       "          0.05202997],\n",
       "         ...,\n",
       "         [0.04343206, 0.25699807, 0.35647539, 0.10927793, 0.17990629,\n",
       "          0.05391026],\n",
       "         [0.02695285, 0.1834832 , 0.45522165, 0.12445093, 0.12186262,\n",
       "          0.08802875],\n",
       "         [0.06900415, 0.29523952, 0.29580842, 0.0807735 , 0.17717403,\n",
       "          0.08200037]]),\n",
       "  array(['happy', 'happy', 'happy', ..., 'happy', 'happy', 'happy'],\n",
       "        dtype=object),\n",
       "  15585    2\n",
       "  42829    3\n",
       "  912      2\n",
       "  16067    0\n",
       "  14171    4\n",
       "          ..\n",
       "  33481    1\n",
       "  1223     2\n",
       "  33816    4\n",
       "  36496    2\n",
       "  30638    1\n",
       "  Name: emotion, Length: 9526, dtype: int64]}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "resultsjb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_bin = joblib.load(\"../dump/results_bin_BERT.joblib\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'LogisticRegression': [None,\n",
       "  array([[5.62183090e-04, 1.01398679e-03, 9.81819956e-01, 1.47508787e-02,\n",
       "          1.60689681e-03, 2.46098243e-04],\n",
       "         [3.92704291e-04, 2.94389445e-02, 9.38348570e-01, 2.90768300e-02,\n",
       "          8.24896247e-04, 1.91805543e-03],\n",
       "         [2.76624749e-03, 3.81719045e-04, 1.71632688e-02, 9.75651276e-01,\n",
       "          4.03139103e-03, 6.09737489e-06],\n",
       "         ...,\n",
       "         [1.01071205e-01, 3.81638760e-02, 4.21576510e-02, 2.40010505e-02,\n",
       "          7.72579471e-01, 2.20267456e-02],\n",
       "         [9.06399546e-02, 1.47631251e-02, 3.25881255e-01, 3.97076191e-01,\n",
       "          1.69809357e-01, 1.83011721e-03],\n",
       "         [2.91848951e-02, 9.95669511e-03, 3.95084881e-01, 3.80706266e-01,\n",
       "          1.78828956e-01, 6.23830710e-03]]),\n",
       "  array([2, 2, 3, ..., 4, 3, 2]),\n",
       "  15989    2\n",
       "  19364    2\n",
       "  7635     3\n",
       "  9802     4\n",
       "  2920     2\n",
       "          ..\n",
       "  8960     3\n",
       "  21336    1\n",
       "  7427     4\n",
       "  17730    3\n",
       "  4721     2\n",
       "  Name: emotion, Length: 4292, dtype: int64],\n",
       " 'SGDClassifier': [None,\n",
       "  None,\n",
       "  array([4, 2, 2, ..., 2, 2, 2]),\n",
       "  12405    0\n",
       "  4495     2\n",
       "  826      2\n",
       "  15795    2\n",
       "  19842    0\n",
       "          ..\n",
       "  512      0\n",
       "  9628     2\n",
       "  11060    2\n",
       "  11619    2\n",
       "  10457    2\n",
       "  Name: emotion, Length: 4292, dtype: int64],\n",
       " 'RandomForest': [None,\n",
       "  array([[0.17, 0.08, 0.36, 0.09, 0.27, 0.03],\n",
       "         [0.13, 0.09, 0.19, 0.05, 0.53, 0.01],\n",
       "         [0.15, 0.16, 0.24, 0.05, 0.38, 0.02],\n",
       "         ...,\n",
       "         [0.08, 0.05, 0.64, 0.05, 0.14, 0.04],\n",
       "         [0.08, 0.07, 0.02, 0.01, 0.81, 0.01],\n",
       "         [0.07, 0.14, 0.2 , 0.05, 0.51, 0.03]]),\n",
       "  array([2, 4, 4, ..., 2, 4, 4]),\n",
       "  16122    0\n",
       "  10397    4\n",
       "  2048     2\n",
       "  15813    0\n",
       "  9733     2\n",
       "          ..\n",
       "  4032     3\n",
       "  17110    3\n",
       "  14984    2\n",
       "  12868    4\n",
       "  15504    4\n",
       "  Name: emotion, Length: 4292, dtype: int64],\n",
       " 'Xgboost': [None,\n",
       "  array([[4.40620214e-01, 3.68476920e-02, 6.67197928e-02, 8.61464441e-03,\n",
       "          4.46656346e-01, 5.41340094e-04],\n",
       "         [2.03404832e-03, 4.75626141e-02, 8.52226794e-01, 1.54428957e-02,\n",
       "          8.22098553e-02, 5.23810566e-04],\n",
       "         [3.36047590e-01, 1.93595830e-02, 2.37289831e-01, 1.65873859e-02,\n",
       "          3.89805824e-01, 9.09789116e-04],\n",
       "         ...,\n",
       "         [9.46342424e-02, 7.70195201e-02, 7.63116717e-01, 4.20039147e-03,\n",
       "          5.94423190e-02, 1.58677273e-03],\n",
       "         [5.06185740e-02, 4.15908592e-03, 8.49069595e-01, 9.11985189e-02,\n",
       "          4.67580091e-03, 2.78429710e-04],\n",
       "         [3.79069038e-02, 1.08163491e-01, 1.07523866e-01, 1.38377994e-02,\n",
       "          7.32044041e-01, 5.23906900e-04]], dtype=float32),\n",
       "  array([4, 2, 4, ..., 2, 2, 4]),\n",
       "  8942     4\n",
       "  3332     2\n",
       "  10307    3\n",
       "  8706     2\n",
       "  8083     2\n",
       "          ..\n",
       "  12038    1\n",
       "  14164    0\n",
       "  8941     1\n",
       "  1089     3\n",
       "  2092     4\n",
       "  Name: emotion, Length: 4292, dtype: int64],\n",
       " 'NeuralNetwork': [None,\n",
       "  array([[1.],\n",
       "         [1.],\n",
       "         [1.],\n",
       "         ...,\n",
       "         [1.],\n",
       "         [1.],\n",
       "         [1.]], dtype=float32),\n",
       "  array([1, 1, 1, ..., 1, 1, 1], dtype=int32),\n",
       "  5404     4\n",
       "  16539    2\n",
       "  6882     2\n",
       "  8728     0\n",
       "  4147     4\n",
       "          ..\n",
       "  12869    4\n",
       "  3675     4\n",
       "  12923    4\n",
       "  4783     4\n",
       "  9258     4\n",
       "  Name: emotion, Length: 4292, dtype: int64]}"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results_bin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([2, 2, 3, ..., 4, 3, 2])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results_bin[\"LogisticRegression\"][2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'LogisticRegression': [LogisticRegression(C=1, max_iter=500, multi_class='multinomial', n_jobs=-1,\n",
       "                     solver='saga'),\n",
       "  array([[2.79707145e-02, 1.59962565e-01, 2.47402722e-03, 1.89044874e-03,\n",
       "          8.07208240e-01, 4.94063890e-04],\n",
       "         [1.17768683e-02, 3.54708910e-01, 8.21517184e-02, 1.85649972e-02,\n",
       "          4.63497519e-01, 6.92999586e-02],\n",
       "         [1.83975045e-02, 1.06561504e-01, 6.05407774e-01, 8.82487670e-02,\n",
       "          1.00317955e-01, 8.10665861e-02],\n",
       "         ...,\n",
       "         [6.81919104e-04, 8.12693238e-02, 7.08899796e-01, 1.47259817e-01,\n",
       "          4.89583649e-02, 1.29307508e-02],\n",
       "         [1.23182163e-02, 6.89486265e-02, 2.27198750e-01, 6.56326354e-01,\n",
       "          1.75954625e-02, 1.76126696e-02],\n",
       "         [7.23050356e-01, 3.35451402e-02, 8.10149033e-03, 9.34884045e-03,\n",
       "          2.19858065e-01, 6.09621825e-03]], dtype=float32),\n",
       "  array(['sadness', 'sadness', 'happy', ..., 'happy', 'love', 'anger'],\n",
       "        dtype=object),\n",
       "  10936    1\n",
       "  30676    4\n",
       "  36012    1\n",
       "  44996    2\n",
       "  23342    1\n",
       "          ..\n",
       "  32231    4\n",
       "  20838    4\n",
       "  45385    2\n",
       "  1360     1\n",
       "  19441    0\n",
       "  Name: emotion, Length: 9526, dtype: int64],\n",
       " 'SGDClassifier': [SGDClassifier(alpha=0.001, max_iter=500, n_jobs=-1),\n",
       "  None,\n",
       "  array(['happy', 'happy', 'sadness', ..., 'happy', 'happy', 'anger'],\n",
       "        dtype=object),\n",
       "  11054    3\n",
       "  33432    2\n",
       "  3563     4\n",
       "  1183     2\n",
       "  1884     4\n",
       "          ..\n",
       "  8796     2\n",
       "  31955    1\n",
       "  12745    4\n",
       "  44682    2\n",
       "  15661    1\n",
       "  Name: emotion, Length: 9526, dtype: int64],\n",
       " 'RandomForest': [RandomForestClassifier(criterion='entropy', min_samples_leaf=2,\n",
       "                         min_samples_split=8, n_estimators=1000, n_jobs=-1),\n",
       "  array([[0.04169398, 0.14711194, 0.51337929, 0.11316504, 0.10704395,\n",
       "          0.07760579],\n",
       "         [0.03544899, 0.23556206, 0.35172026, 0.14632224, 0.16932527,\n",
       "          0.06162118],\n",
       "         [0.05847385, 0.11306005, 0.49437453, 0.16715774, 0.11490387,\n",
       "          0.05202997],\n",
       "         ...,\n",
       "         [0.04343206, 0.25699807, 0.35647539, 0.10927793, 0.17990629,\n",
       "          0.05391026],\n",
       "         [0.02695285, 0.1834832 , 0.45522165, 0.12445093, 0.12186262,\n",
       "          0.08802875],\n",
       "         [0.06900415, 0.29523952, 0.29580842, 0.0807735 , 0.17717403,\n",
       "          0.08200037]]),\n",
       "  array(['happy', 'happy', 'happy', ..., 'happy', 'happy', 'happy'],\n",
       "        dtype=object),\n",
       "  15585    2\n",
       "  42829    3\n",
       "  912      2\n",
       "  16067    0\n",
       "  14171    4\n",
       "          ..\n",
       "  33481    1\n",
       "  1223     2\n",
       "  33816    4\n",
       "  36496    2\n",
       "  30638    1\n",
       "  Name: emotion, Length: 9526, dtype: int64]}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "resultsjb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 1, 2, 3, 4, 5])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.unique(resultsjb['LogisticRegression'][3].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-05-21 11:38:40.362 INFO    sentence_transformers.SentenceTransformer: Load pretrained SentenceTransformer: paraphrase-MiniLM-L6-v2\n",
      "2021-05-21 11:38:40.364 INFO    sentence_transformers.SentenceTransformer: Did not find folder paraphrase-MiniLM-L6-v2\n",
      "2021-05-21 11:38:40.366 INFO    sentence_transformers.SentenceTransformer: Search model on server: http://sbert.net/models/paraphrase-MiniLM-L6-v2.zip\n",
      "2021-05-21 11:38:40.367 INFO    sentence_transformers.SentenceTransformer: Load SentenceTransformer from folder: /home/apprenant/.cache/torch/sentence_transformers/sbert.net_models_paraphrase-MiniLM-L6-v2\n",
      "2021-05-21 11:38:40.368 WARNING sentence_transformers.SentenceTransformer: You try to use a model that was created with version 1.2.0, however, your version is 1.1.0. This might cause unexpected behavior or errors. In that case, try to update to the latest version.\n",
      "\n",
      "\n",
      "\n",
      "2021-05-21 11:38:41.234 INFO    sentence_transformers.SentenceTransformer: Use pytorch device: cpu\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d89c6b15a9914a248e29adfe851fd79c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value='Batches'), FloatProgress(value=0.0, max=1489.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/apprenant/anaconda3/lib/python3.8/site-packages/sklearn/linear_model/_sag.py:329: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\"The max_iter was reached which means \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train R2 = -0.298; Train RMSE 1.591\n",
      "Test R2  = -0.31; Test RMSE 1.601\n",
      "Train R2 = -0.371; Train RMSE 1.639\n",
      "Test R2  = -0.414; Test RMSE 1.649\n",
      "Train R2 = 0.98; Train RMSE 0.199\n",
      "Test R2  = -0.297; Test RMSE 1.592\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/apprenant/anaconda3/lib/python3.8/site-packages/xgboost/sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[11:52:47] WARNING: ../src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "Train R2 = 0.857; Train RMSE 0.527\n",
      "Test R2  = -0.36; Test RMSE 1.63\n",
      "Epoch 1/10\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "in user code:\n\n    /home/apprenant/anaconda3/lib/python3.8/site-packages/tensorflow/python/keras/engine/training.py:805 train_function  *\n        return step_function(self, iterator)\n    /home/apprenant/anaconda3/lib/python3.8/site-packages/tensorflow/python/keras/engine/training.py:795 step_function  **\n        outputs = model.distribute_strategy.run(run_step, args=(data,))\n    /home/apprenant/anaconda3/lib/python3.8/site-packages/tensorflow/python/distribute/distribute_lib.py:1259 run\n        return self._extended.call_for_each_replica(fn, args=args, kwargs=kwargs)\n    /home/apprenant/anaconda3/lib/python3.8/site-packages/tensorflow/python/distribute/distribute_lib.py:2730 call_for_each_replica\n        return self._call_for_each_replica(fn, args, kwargs)\n    /home/apprenant/anaconda3/lib/python3.8/site-packages/tensorflow/python/distribute/distribute_lib.py:3417 _call_for_each_replica\n        return fn(*args, **kwargs)\n    /home/apprenant/anaconda3/lib/python3.8/site-packages/tensorflow/python/keras/engine/training.py:788 run_step  **\n        outputs = model.train_step(data)\n    /home/apprenant/anaconda3/lib/python3.8/site-packages/tensorflow/python/keras/engine/training.py:755 train_step\n        loss = self.compiled_loss(\n    /home/apprenant/anaconda3/lib/python3.8/site-packages/tensorflow/python/keras/engine/compile_utils.py:203 __call__\n        loss_value = loss_obj(y_t, y_p, sample_weight=sw)\n    /home/apprenant/anaconda3/lib/python3.8/site-packages/tensorflow/python/keras/losses.py:152 __call__\n        losses = call_fn(y_true, y_pred)\n    /home/apprenant/anaconda3/lib/python3.8/site-packages/tensorflow/python/keras/losses.py:256 call  **\n        return ag_fn(y_true, y_pred, **self._fn_kwargs)\n    /home/apprenant/anaconda3/lib/python3.8/site-packages/tensorflow/python/util/dispatch.py:201 wrapper\n        return target(*args, **kwargs)\n    /home/apprenant/anaconda3/lib/python3.8/site-packages/tensorflow/python/keras/losses.py:1608 binary_crossentropy\n        K.binary_crossentropy(y_true, y_pred, from_logits=from_logits), axis=-1)\n    /home/apprenant/anaconda3/lib/python3.8/site-packages/tensorflow/python/util/dispatch.py:201 wrapper\n        return target(*args, **kwargs)\n    /home/apprenant/anaconda3/lib/python3.8/site-packages/tensorflow/python/keras/backend.py:4979 binary_crossentropy\n        return nn.sigmoid_cross_entropy_with_logits(labels=target, logits=output)\n    /home/apprenant/anaconda3/lib/python3.8/site-packages/tensorflow/python/util/dispatch.py:201 wrapper\n        return target(*args, **kwargs)\n    /home/apprenant/anaconda3/lib/python3.8/site-packages/tensorflow/python/ops/nn_impl.py:173 sigmoid_cross_entropy_with_logits\n        raise ValueError(\"logits and labels must have the same shape (%s vs %s)\" %\n\n    ValueError: logits and labels must have the same shape ((100, 6) vs (100, 1))\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-3-f11c74f71e5d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mresults2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel_results_multi\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf_dataK_ready\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/PycharmProjects/TP--Find_the_Feel/src/Machine_learning_advanced.py\u001b[0m in \u001b[0;36mmodel_results_multi\u001b[0;34m(df_dataK)\u001b[0m\n\u001b[1;32m    142\u001b[0m             \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlayers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDense\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m6\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mactivation\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'softmax'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    143\u001b[0m             \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"adam\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"binary_crossentropy\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmetrics\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'accuracy'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 144\u001b[0;31m             \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    145\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    146\u001b[0m             \u001b[0my_train_pred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict_classes\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.8/site-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1098\u001b[0m                 _r=1):\n\u001b[1;32m   1099\u001b[0m               \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_train_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1100\u001b[0;31m               \u001b[0mtmp_logs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1101\u001b[0m               \u001b[0;32mif\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshould_sync\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1102\u001b[0m                 \u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masync_wait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.8/site-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    826\u001b[0m     \u001b[0mtracing_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    827\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mtrace\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTrace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_name\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mtm\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 828\u001b[0;31m       \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    829\u001b[0m       \u001b[0mcompiler\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"xla\"\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_experimental_compile\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34m\"nonXla\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    830\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.8/site-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    869\u001b[0m       \u001b[0;31m# This is the first call of __call__, so we have to initialize.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    870\u001b[0m       \u001b[0minitializers\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 871\u001b[0;31m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_initialize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0madd_initializers_to\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minitializers\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    872\u001b[0m     \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    873\u001b[0m       \u001b[0;31m# At this point we know that the initialization is complete (or less\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.8/site-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m_initialize\u001b[0;34m(self, args, kwds, add_initializers_to)\u001b[0m\n\u001b[1;32m    723\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_graph_deleter\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mFunctionDeleter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_lifted_initializer_graph\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    724\u001b[0m     self._concrete_stateful_fn = (\n\u001b[0;32m--> 725\u001b[0;31m         self._stateful_fn._get_concrete_function_internal_garbage_collected(  # pylint: disable=protected-access\n\u001b[0m\u001b[1;32m    726\u001b[0m             *args, **kwds))\n\u001b[1;32m    727\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.8/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_get_concrete_function_internal_garbage_collected\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   2967\u001b[0m       \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2968\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_lock\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2969\u001b[0;31m       \u001b[0mgraph_function\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_maybe_define_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2970\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mgraph_function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2971\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.8/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_maybe_define_function\u001b[0;34m(self, args, kwargs)\u001b[0m\n\u001b[1;32m   3359\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3360\u001b[0m           \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_function_cache\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmissed\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcall_context_key\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3361\u001b[0;31m           \u001b[0mgraph_function\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_create_graph_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3362\u001b[0m           \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_function_cache\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprimary\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcache_key\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgraph_function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3363\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.8/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_create_graph_function\u001b[0;34m(self, args, kwargs, override_flat_arg_shapes)\u001b[0m\n\u001b[1;32m   3194\u001b[0m     \u001b[0marg_names\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbase_arg_names\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mmissing_arg_names\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3195\u001b[0m     graph_function = ConcreteFunction(\n\u001b[0;32m-> 3196\u001b[0;31m         func_graph_module.func_graph_from_py_func(\n\u001b[0m\u001b[1;32m   3197\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_name\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3198\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_python_function\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.8/site-packages/tensorflow/python/framework/func_graph.py\u001b[0m in \u001b[0;36mfunc_graph_from_py_func\u001b[0;34m(name, python_func, args, kwargs, signature, func_graph, autograph, autograph_options, add_control_dependencies, arg_names, op_return_value, collections, capture_by_value, override_flat_arg_shapes)\u001b[0m\n\u001b[1;32m    988\u001b[0m         \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moriginal_func\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_decorator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munwrap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpython_func\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    989\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 990\u001b[0;31m       \u001b[0mfunc_outputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpython_func\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mfunc_args\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mfunc_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    991\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    992\u001b[0m       \u001b[0;31m# invariant: `func_outputs` contains only Tensors, CompositeTensors,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.8/site-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36mwrapped_fn\u001b[0;34m(*args, **kwds)\u001b[0m\n\u001b[1;32m    632\u001b[0m             \u001b[0mxla_context\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mExit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    633\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 634\u001b[0;31m           \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mweak_wrapped_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__wrapped__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    635\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    636\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.8/site-packages/tensorflow/python/framework/func_graph.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    975\u001b[0m           \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint:disable=broad-except\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    976\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"ag_error_metadata\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 977\u001b[0;31m               \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mag_error_metadata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_exception\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    978\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    979\u001b[0m               \u001b[0;32mraise\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: in user code:\n\n    /home/apprenant/anaconda3/lib/python3.8/site-packages/tensorflow/python/keras/engine/training.py:805 train_function  *\n        return step_function(self, iterator)\n    /home/apprenant/anaconda3/lib/python3.8/site-packages/tensorflow/python/keras/engine/training.py:795 step_function  **\n        outputs = model.distribute_strategy.run(run_step, args=(data,))\n    /home/apprenant/anaconda3/lib/python3.8/site-packages/tensorflow/python/distribute/distribute_lib.py:1259 run\n        return self._extended.call_for_each_replica(fn, args=args, kwargs=kwargs)\n    /home/apprenant/anaconda3/lib/python3.8/site-packages/tensorflow/python/distribute/distribute_lib.py:2730 call_for_each_replica\n        return self._call_for_each_replica(fn, args, kwargs)\n    /home/apprenant/anaconda3/lib/python3.8/site-packages/tensorflow/python/distribute/distribute_lib.py:3417 _call_for_each_replica\n        return fn(*args, **kwargs)\n    /home/apprenant/anaconda3/lib/python3.8/site-packages/tensorflow/python/keras/engine/training.py:788 run_step  **\n        outputs = model.train_step(data)\n    /home/apprenant/anaconda3/lib/python3.8/site-packages/tensorflow/python/keras/engine/training.py:755 train_step\n        loss = self.compiled_loss(\n    /home/apprenant/anaconda3/lib/python3.8/site-packages/tensorflow/python/keras/engine/compile_utils.py:203 __call__\n        loss_value = loss_obj(y_t, y_p, sample_weight=sw)\n    /home/apprenant/anaconda3/lib/python3.8/site-packages/tensorflow/python/keras/losses.py:152 __call__\n        losses = call_fn(y_true, y_pred)\n    /home/apprenant/anaconda3/lib/python3.8/site-packages/tensorflow/python/keras/losses.py:256 call  **\n        return ag_fn(y_true, y_pred, **self._fn_kwargs)\n    /home/apprenant/anaconda3/lib/python3.8/site-packages/tensorflow/python/util/dispatch.py:201 wrapper\n        return target(*args, **kwargs)\n    /home/apprenant/anaconda3/lib/python3.8/site-packages/tensorflow/python/keras/losses.py:1608 binary_crossentropy\n        K.binary_crossentropy(y_true, y_pred, from_logits=from_logits), axis=-1)\n    /home/apprenant/anaconda3/lib/python3.8/site-packages/tensorflow/python/util/dispatch.py:201 wrapper\n        return target(*args, **kwargs)\n    /home/apprenant/anaconda3/lib/python3.8/site-packages/tensorflow/python/keras/backend.py:4979 binary_crossentropy\n        return nn.sigmoid_cross_entropy_with_logits(labels=target, logits=output)\n    /home/apprenant/anaconda3/lib/python3.8/site-packages/tensorflow/python/util/dispatch.py:201 wrapper\n        return target(*args, **kwargs)\n    /home/apprenant/anaconda3/lib/python3.8/site-packages/tensorflow/python/ops/nn_impl.py:173 sigmoid_cross_entropy_with_logits\n        raise ValueError(\"logits and labels must have the same shape (%s vs %s)\" %\n\n    ValueError: logits and labels must have the same shape ((100, 6) vs (100, 1))\n"
     ]
    }
   ],
   "source": [
    "results2 = model_results_multi(df_dataK_ready)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from Machine_learningMVP import model_results\n",
    "\n",
    "results = model_results()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len('str')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import f1_score, precision_score, recall_score, roc_curve, auc\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_true = results['LogisticRegression'][2]\n",
    "y_pred_proba = results['LogisticRegression'][1]\n",
    "fpr_keras, tpr_keras, thresholds_keras = roc_curve(y_true, y_pred_proba[:, 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "thresholds_keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "def tokenize(text):\n",
    "    \"\"\"Tokenize text\"\"\"\n",
    "    tokens = word_tokenize(text)\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "resultattoken = tokenize(\"i'm happy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "resultattoken"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens = resultattoken"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "table = str.maketrans('', '', string.punctuation)\n",
    "stripped = [w.translate(table) for w in tokens]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stripped"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "words = [word for word in stripped if word.isalpha()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"a\".isalpha()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stopz = 0123456789\"#$%&'()*+,-/:;@[]^_`{|}~\n",
    "         \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lemmatizer(text):\n",
    "    \"\"\"Apply Wordnet lemmatizer to text (go to root word)\"\"\"\n",
    "    wnl = WordNetLemmatizer()\n",
    "    text = [wnl.lemmatize(word) for word in text.split()]\n",
    "    return \" \".join(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import WordNetLemmatizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lemmatizer(\"do does did done doesn't\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lower_case(text): \n",
    "    return text.lower()\n",
    "\n",
    "def remove_any_numbers(text):\n",
    "    \"\"\"Remove all independant numbers and all numbers contained in other words\"\"\"\n",
    "    text = [word for word in text.split() if not any(c.isdigit() for c in word)]\n",
    "    return \" \".join(text)\n",
    "\n",
    "def separate_ponctuation(text):\n",
    "    for word in text.split():\n",
    "        if \n",
    "    blabla...adefef => blabla ... adefef"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "auc_keras = auc(fpr_keras, tpr_keras)\n",
    "auc_keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_true"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_proba"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fpr_keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tpr_keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "def model_used(df, model, stop):\n",
    "    \"\"\"Given a model choice, return the model and the computed matrix\"\"\"\n",
    "    if model == 'Tfidf':\n",
    "        tf = TfidfVectorizer(analyzer='word', ngram_range=(1, 2), min_df=0, stop_words=stop)\n",
    "        tfidf_matrix = tf.fit_transform(df['text'])\n",
    "        return tf, tfidf_matrix\n",
    "    elif model == 'CountVectorizer':\n",
    "        cv = CountVectorizer(analyzer='word', ngram_range=(1, 2), min_df=0, stop_words=stop)\n",
    "        matrix = cv.fit_transform(df['text'])\n",
    "        return cv, matrix\n",
    "    elif model == 'BERT':\n",
    "#         bert = SentenceTransformer('distiluse-base-multilingual-cased-v1') # Multilingue\n",
    "#         bert = SentenceTransformer('average_word_embeddings_glove.6B.300d') # + rapide\n",
    "        bert = SentenceTransformer('paraphrase-MiniLM-L6-v2') # Meilleur score en théorie, à vérifier sur nos données\n",
    "        matrix = bert.encode(df['text'].astype('str'), show_progress_bar=True)\n",
    "        return bert, matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sys.path.append(\"../src\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from Machine_learningMVP import df_dataK\n",
    "\n",
    "STOPWORDS.update(['feel','feeling','im',\",\",\"t\",\"u\",\"2\",\"'\",\"&amp;\",\"-\",\"...\",\"s\"])\n",
    "\n",
    "model_used(df_dataK, 'BERT', STOPWORDS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras import layers, preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_dataKKeras = df_dataK[['text']].copy()\n",
    "tk = preprocessing.text.Tokenizer()\n",
    "\n",
    "tk.fit_on_texts(df_dataKKeras['text'])\n",
    "X = tk.texts_to_matrix(df_dataKKeras['text'], mode='tfidf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "400000*300 > 30522*384"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(X[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# ------------------------------------------ Import ------------------------------------------\n",
    "#Set Relative Path\n",
    "import sys\n",
    "sys.path.append(\"../src\")\n",
    "# Importing the libraries\n",
    "\n",
    "# import streamlit as st\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from Machine_learningMVP import y_pred_proba, y_true, model_results\n",
    "\n",
    "# ------------------------------------------ Code ------------------------------------------\n",
    "\n",
    "from sklearn.metrics import f1_score, precision_score, recall_score\n",
    "from sklearn.metrics import confusion_matrix, precision_recall_fscore_support, classification_report\n",
    "from Data_Exploratory import df_dataW, df_dataK\n",
    "from wordcloud import STOPWORDS\n",
    "from collections import Counter\n",
    "\n",
    "\n",
    "\n",
    "# @st.cache(allow_output_mutation=True)\n",
    "def extract_most_used_word(text, stopwords=STOPWORDS, words=1):\n",
    "    \"\"\"Return the wanted number of most used word of a given text and their number of occurences\"\"\"\n",
    "    words_list=[]\n",
    "    # iterate through the csv file\n",
    "    for val in text:\n",
    "        # typecaste each val to string\n",
    "        val = str(val)\n",
    "        # split the value\n",
    "        tokens = val.split()\n",
    "        # Converts each token into lowercase\n",
    "        for i in range(len(tokens)):\n",
    "            tokens[i] = tokens[i].lower()\n",
    "            if not tokens[i] in stopwords:\n",
    "                words_list.append(tokens[i])\n",
    "    occurence_count = Counter(words_list)\n",
    "    return np.array(occurence_count.most_common())[:words]\n",
    "\n",
    "\n",
    "def word_global_repartition_plot(df):\n",
    "    data = df['Emotion'].value_counts()\n",
    "    ax = sns.barplot(x=data.index, y=data.values)\n",
    "    fig = plt.gcf()\n",
    "    # Modifie la taille du graphique\n",
    "    fig.set_size_inches(10, 4)\n",
    "    # Ajout du titre\n",
    "    fig.suptitle(\"Répartition des émotions\", fontsize=18)\n",
    "    # Ajout des labels pour les axes x et y\n",
    "    plt.xlabel(\"Emotion\", fontsize=20);\n",
    "    plt.ylabel(\"Occurences\", fontsize=20, rotation=90)\n",
    "    st.pyplot(fig)\n",
    "    plt.close()\n",
    "\n",
    "\n",
    "\n",
    "def word_local_repartition_plot(df, words=20):\n",
    "    for emotion in df['Emotion'].unique():\n",
    "        x = extract_most_used_word(df[df['Emotion'] == emotion]['Text'], words=words)[:, 0]\n",
    "        y = extract_most_used_word(df[df['Emotion'] == emotion]['Text'], words=words)[:, 1].astype('int')\n",
    "\n",
    "        ax = sns.barplot(x=x, y=y)\n",
    "        fig = plt.gcf()\n",
    "        # Modifie la taille du graphique\n",
    "        fig.set_size_inches(10, 4)\n",
    "        # Ajout du titre\n",
    "        fig.suptitle(\"Répartition pour \" + emotion, fontsize=18)\n",
    "        # Ajout des labels pour les axes x et y\n",
    "        plt.xlabel(\"Mots\", fontsize=20);\n",
    "        plt.ylabel(\"Occurences\", fontsize=20, rotation=90)\n",
    "        st.pyplot(fig)\n",
    "        plt.close()\n",
    "\n",
    "\n",
    "# Rappel et précision en fonction de la proba\n",
    "# @st.cache(allow_output_mutation=True)\n",
    "def rappel_precision_f1(prob, ytrue, steps=101):\n",
    "    \"\"\"\n",
    "    Return the recall, precision and f1_score and an array containing each probability for which the metrics where calculated\n",
    "    Steps represents the number of probability taken\n",
    "    \"\"\"\n",
    "    x = np.linspace(0,1,steps)\n",
    "    rap = []\n",
    "    pre = []\n",
    "    f1 = []\n",
    "    for val in x :\n",
    "        ypred = [proba >= val for proba in prob]\n",
    "        rap.append(recall_score(ytrue,ypred))\n",
    "        pre.append(precision_score(ytrue,ypred))\n",
    "        f1.append(f1_score(ytrue,ypred))\n",
    "    return rap, pre, f1, x\n",
    "\n",
    "\n",
    "def prediction():\n",
    "    for key, values in results.items():\n",
    "        st.header(\"Résultat de \"+ str(key))\n",
    "        st.subheader(\"Courbe de rappel et précision\")\n",
    "        rappel, precision, f1, x = rappel_precision_f1(values[1][:, 1], values[3].values)\n",
    "        d = {'x': x, 'f1': f1, 'rappel': rappel, 'precision': precision}\n",
    "        chart_data = pd.DataFrame(data=d)\n",
    "        chart_data = chart_data.set_index('x')\n",
    "        st.line_chart(chart_data)\n",
    "        st.subheader(\"Matrice de confusion\")\n",
    "        y_pred = [val > 0.5 for val in y_pred_proba[:, 1]]\n",
    "        tabl = pd.DataFrame(confusion_matrix(y_pred, y_true.values), index = ['Negative', 'Positive'], columns=['Negative', 'Positive'])\n",
    "        ax = sns.heatmap(tabl, annot=True, fmt='d')\n",
    "        fig = plt.gcf()\n",
    "        plt.close()\n",
    "        st.pyplot(fig)\n",
    "    st.subheader(\"Tableaux de métriques\")\n",
    "    tabl = pd.DataFrame(precision_recall_fscore_support(y_pred, y_true.values), columns = ['Régression logistique : Negative', 'Régression logistique : Positive'], index=['Precision', 'Rappel', 'f1', 'support']).T\n",
    "    st.write(tabl)\n",
    "\n",
    "\n",
    "def data_analysis():\n",
    "    st.header(\"Analyse de la donnée\")\n",
    "    st.subheader(\"Sur le premier jeu de donnée\")\n",
    "    word_global_repartition_plot(df_dataW)\n",
    "    word_local_repartition_plot(df_dataW)\n",
    "    st.subheader(\"Sur le second jeu de donnée\")\n",
    "    word_global_repartition_plot(df_dataK)\n",
    "    word_local_repartition_plot(df_dataK)\n",
    "\n",
    "\n",
    "results = model_results()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = pd.DataFrame([['a', 1], ['b', 2]], columns=['letter', 'number'])\n",
    "\n",
    "df2 = pd.DataFrame([['c', 3], ['d', 4]], columns=['letter', 'number'])\n",
    "\n",
    "df3 = pd.DataFrame([['e', 5], ['f', 6]], columns=['letter', 'number'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dflist = [df1, df2, df3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.concat([df1], keys=['df1'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dflist[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "number = 0.2\n",
    "if 0 < number < 1:\n",
    "    print('Validé')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from app import results as resultsimp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from tensorflow.keras import layers, preprocessing\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras.models import Sequential\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "df_dataK = pd.read_csv(r'../data/d03_cleaned_data/CleanKaggle.csv')\n",
    "df_dataK.head()\n",
    "STOPWORDS.update(['feel','feeling','im',\",\",\"t\",\"u\",\"2\",\"'\",\"&amp;\",\"-\",\"...\",\"s\"])\n",
    "\n",
    "# Set X and y\n",
    "tf = TfidfVectorizer(analyzer='word', ngram_range=(1, 2), min_df=0, stop_words=STOPWORDS)\n",
    "tfidf_matrix = tf.fit_transform(df_dataK['text'])\n",
    "X = tfidf_matrix\n",
    "\n",
    "Labelenc = LabelEncoder()\n",
    "df_dataK['emotion'] = Labelenc.fit_transform(df_dataK['emotion'])\n",
    "y = df_dataK['emotion']\n",
    "df_dataKKeras = df_dataK[['text']].copy()\n",
    "tk = preprocessing.text.Tokenizer()\n",
    "\n",
    "tk.fit_on_texts(df_dataKKeras['text'])\n",
    "X = tk.texts_to_matrix(df_dataKKeras['text'], mode='tfidf')\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, train_size=0.8, random_state=None)\n",
    "model =  Sequential()\n",
    "model.add(layers.Dense(16, activation='relu', input_shape=[X_train.shape[1]]))\n",
    "model.add(layers.Dense(8, activation='relu'))\n",
    "model.add(layers.Dense(4, activation='relu'))\n",
    "model.add(layers.Dense(1, activation='relu'))\n",
    "model.compile(optimizer=\"adam\", loss=\"binary_crossentropy\")\n",
    "model.fit(X_train, y_train, epochs=10, batch_size=10)\n",
    "y_test_pred = model.predict_classes(X_test)\n",
    "y_test_proba = model.predict_proba(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test_proba"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "temporaire = np.array([x[0] for x in y_test_proba])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(temporaire)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "resultsimp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "resultssave=results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(results.values())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for key, values in results.items():\n",
    "#     print(values[0])\n",
    "    print(values[1])\n",
    "    print('\\n\\n SEP \\n\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results['LogisticRegression'][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for key, values in results.items():\n",
    "    print(values[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for key, values in results.items():\n",
    "    print(results[key])\n",
    "    print('SEP')\n",
    "    print(values)\n",
    "#     for value in results[key]:\n",
    "#         print(type(value))\n",
    "#         print(value)\n",
    "\n",
    "#     for value in values:\n",
    "#         print(type(value))\n",
    "#         print(value)\n",
    "    print('Changement clé')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d = {'x': [1,2], 'y': [3,4], 'z': [5,6]} \n",
    "count=0\n",
    "for key,values in d.items():\n",
    "    print(\"Compte\" +str(count))\n",
    "    count=+1\n",
    "    print(key)\n",
    "    print(d[key])\n",
    "    print(values[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list(d.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_kaggle = pd.read_csv('../data/d01_raw/Emotion_Kaggle.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_kaggle.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_kaggle.head"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_kaggle['Emotion'].value_counts(normalize=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = df_kaggle['Emotion'].value_counts().index\n",
    "y = df_kaggle['Emotion'].value_counts().values\n",
    "\n",
    "ax=sns.barplot(x=x ,y=y)\n",
    "fig = plt.gcf()\n",
    "# Modifie la taille du graphique\n",
    "fig.set_size_inches(10,4)\n",
    "# Ajout du titre\n",
    "fig.suptitle(\"Répartition des émotions\", fontsize=18)\n",
    "# Ajout des labels pour les axes x et y\n",
    "plt.xlabel(\"Emotion\", fontsize=20);\n",
    "plt.ylabel(\"Occurences\", fontsize=20, rotation=90);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = df_kaggle.groupby(\"Emotion\").Emotion.count().sort_values()\n",
    "data.plot.pie(autopct=\"%.1f%%\", radius=3);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "comment_words = ''\n",
    "stopwords = set(STOPWORDS)\n",
    "  \n",
    "# iterate through the csv file\n",
    "for val in df_kaggle['Text']:\n",
    "      \n",
    "    # typecaste each val to string\n",
    "    val = str(val)\n",
    "  \n",
    "    # split the value\n",
    "    tokens = val.split()\n",
    "      \n",
    "    # Converts each token into lowercase\n",
    "    for i in range(len(tokens)):\n",
    "        tokens[i] = tokens[i].lower()\n",
    "      \n",
    "    comment_words += \" \".join(tokens)+\" \"\n",
    "  \n",
    "wordcloud = WordCloud(width = 800, height = 800,\n",
    "                background_color ='white',\n",
    "                stopwords = stopwords,\n",
    "                min_font_size = 10).generate(comment_words)\n",
    "  \n",
    "# plot the WordCloud image                       \n",
    "plt.figure(figsize = (8, 8), facecolor = None)\n",
    "plt.imshow(wordcloud)\n",
    "plt.axis(\"off\")\n",
    "plt.tight_layout(pad = 0)\n",
    "  \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "comment_words = ''\n",
    "stopwords = set(STOPWORDS)\n",
    "\n",
    "for emotion in df_kaggle[\"Emotion\"].unique():\n",
    "    # iterate through the csv file\n",
    "    for val in df_kaggle[df_kaggle[\"Emotion\"] == emotion]['Text']:\n",
    "\n",
    "        # typecaste each val to string\n",
    "        val = str(val)\n",
    "\n",
    "        # split the value\n",
    "        tokens = val.split()\n",
    "\n",
    "        # Converts each token into lowercase\n",
    "        for i in range(len(tokens)):\n",
    "            tokens[i] = tokens[i].lower()\n",
    "\n",
    "        comment_words += \" \".join(tokens)+\" \"\n",
    "\n",
    "    wordcloud = WordCloud(width = 800, height = 800,\n",
    "                    background_color ='white',\n",
    "                    stopwords = stopwords,\n",
    "                    min_font_size = 10).generate(comment_words)\n",
    "\n",
    "    # plot the WordCloud image                       \n",
    "    plt.figure(figsize = (8, 8), facecolor = None)\n",
    "    plt.imshow(wordcloud)\n",
    "    plt.axis(\"off\")\n",
    "    plt.tight_layout(pad = 0)\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wordcloud.words_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    print(5)\n",
    "    print('a'+5)\n",
    "except:\n",
    "    print('Tristesse')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_most_used_word(text, stopwords=STOPWORDS, words=1, limit=False):\n",
    "    \"\"\"\n",
    "    Return the wanted number of most used word of a given text and their number of occurences\n",
    "    If limit is set to a value, it will return all words that appears more than limit value\n",
    "    \"\"\"\n",
    "    words_list=[]\n",
    "    # iterate through the csv file\n",
    "    for val in text:\n",
    "        # typecaste each val to string\n",
    "        val = str(val)\n",
    "        # split the value\n",
    "        tokens = val.split()\n",
    "        # Converts each token into lowercase\n",
    "        for i in range(len(tokens)):\n",
    "            tokens[i] = tokens[i].lower()\n",
    "            if not tokens[i] in stopwords:\n",
    "                words_list.append(tokens[i])\n",
    "    occurence_count = Counter(words_list)\n",
    "    occ = np.array(occurence_count.most_common())\n",
    "    if limit:\n",
    "        if int(limit) == limit:\n",
    "            return occ[occ[:,1].astype('int') > limit]\n",
    "        try : \n",
    "            return occ[occ[:,1].astype('int') > limit*text.shape[0]]\n",
    "        except:\n",
    "            return occ[occ[:,1].astype('int') > limit*len(occ)]\n",
    "    return occ[:words]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "occ = extract_most_used_word(df_kaggle['Text'], limit=0.03)\n",
    "occ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "occ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "occ[occ[:,1].astype('int') > 700]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "occ[:,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_most_used_words(df, text_column='Text', emotion_column='Emotion', max_occurences=3, limit=500):\n",
    "    wordlist =[]\n",
    "    for emotion in df[emotion_column].unique():\n",
    "        wordlist.extend(extract_most_used_word(df[text_column], limit=limit)[:,0])\n",
    "    return list(set([i for i in wordlist if wordlist.count(i) > max_occurences]))\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "extract_most_used_words(df_kaggle, limit=0.02)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "set([i for i in mylist if mylist.count(i)>1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(STOPWORDS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "blabla = STOPWORDS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "blabla.update([\"d\", \"e\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "blabla"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if 1:\n",
    "    print('Allo')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "emotionsz = df_kaggle['Emotion'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list(emotionsz)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def word_local_repartition_plot(df, emotion_column='Emotion', text_column='Text', words=20):\n",
    "    emotions = df[emotion_column].unique()\n",
    "    chosen_emotions = df[emotion_column].unique()\n",
    "    count = 0\n",
    "    print(chosen_emotions)\n",
    "    list_cols=[]\n",
    "    for val in range(len(chosen_emotions)//2):\n",
    "        cols = plt.subplots(1, 2,  figsize=(12,12))\n",
    "        list_cols.append(cols)\n",
    "    if len(chosen_emotions)%2:\n",
    "        cols = plt.subplots(1, 2,  figsize=(12,12))\n",
    "        list_cols.append(cols)\n",
    "    return list_cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "subplot = word_local_repartition_plot(df_kaggle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "subplot[0][1][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def word_local_repartition_plot(df, emotion_column='Emotion', text_column='Text', words=20):\n",
    "    with st.sidebar.beta_expander('Répartition par émotion'):\n",
    "        emotions = df[emotion_column].unique()\n",
    "        chosen_emotions = st.multiselect('Emotions', emotions, default=emotions)\n",
    "        fig, axes = plt.subplots(len(chosen_emotions)//2+len(chosen_emotions)%2, 2, figsize=(12,12))\n",
    "        count=0\n",
    "        fig.suptitle(\"Répartition par émotion\", fontweight='bold', size=25)\n",
    "        for emotion in chosen_emotions:\n",
    "            x = extract_most_used_word(df[df[emotion_column] == emotion][text_column], words=words)[:,0]\n",
    "            y = extract_most_used_word(df[df[emotion_column] == emotion][text_column], words=words)[:,1].astype('int')\n",
    "            ax=axes[count//2, count%2]\n",
    "            sns.barplot(ax=ax, x=x ,y=y)\n",
    "            fig2 = plt.gcf()\n",
    "            ax.tick_params('x', labelrotation=90) \n",
    "            # Ajout des labels pour les axes x et y\n",
    "            ax.set_xlabel('Mots', fontsize=18)\n",
    "            ax.set_ylabel('Occurences', fontsize=18)\n",
    "            ax.set_title('Répartition pour ' + emotion, fontweight='bold', size=20)\n",
    "            count=count+1\n",
    "        plt.tight_layout()   \n",
    "        st.pyplot(fig)\n",
    "        plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "words=20\n",
    "for emotion in df_kaggle['Emotion'].unique():\n",
    "    x = extract_most_used_word(df_kaggle[df_kaggle['Emotion'] == emotion]['Text'], words=words)[:,0]\n",
    "    y = extract_most_used_word(df_kaggle[df_kaggle['Emotion'] == emotion]['Text'], words=words)[:,1].astype('int')\n",
    "\n",
    "    ax=sns.barplot(x=x ,y=y)\n",
    "    fig = plt.gcf()\n",
    "    # Modifie la taille du graphique\n",
    "    fig.set_size_inches(10,4)\n",
    "    # Ajout du titre\n",
    "    fig.suptitle(\"Répartition pour \"+emotion, fontsize=18)\n",
    "    # Ajout des labels pour les axes x et y\n",
    "    plt.xlabel(\"Mots\", fontsize=20);\n",
    "    plt.ylabel(\"Occurences\", fontsize=20, rotation=90)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_kaggle[df_kaggle[\"Emotion\"] == \"surprise\"]['Text'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Set Relative Path\n",
    "import sys\n",
    "sys.path.append(\"../src\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from Machine_learningMVP import y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from Machine_learningMVP import y_pred_proba, y_true, y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_true"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_proba[:,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.randn(20,3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import f1_score, precision_score, recall_score\n",
    "\n",
    "\n",
    "def rappel_precision_f1(prob, ytrue, steps=101):\n",
    "    \"\"\"\n",
    "    Return the recall, precision and f1_score and an array containing each probability for which the metrics where calculated\n",
    "    Steps represents the number of probability taken\n",
    "    \"\"\"\n",
    "    x = np.linspace(0,1,steps)\n",
    "    rap = []\n",
    "    pre = []\n",
    "    f1 = []\n",
    "    for val in x :\n",
    "        ypred = [proba >= val for proba in prob]\n",
    "        rap.append(recall_score(ytrue,ypred))\n",
    "        pre.append(precision_score(ytrue,ypred))\n",
    "        f1.append(f1_score(ytrue,ypred))\n",
    "    return rap, pre, f1, x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rappel, precision, f1, x = rappel_precision_f1(y_pred_proba[:,1], y_true.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(f1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d = {'x' : x, 'f1' : f1, 'rappel' : rappel, 'precision' : precision}\n",
    "chart_data = pd.DataFrame(data=d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chart_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chart_data = pd.DataFrame(\n",
    "     np.random.randn(20, 3),\n",
    "     columns=['a', 'b', 'c'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chart_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = [val > 0.5 for val in y_pred_proba[:,1]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "matrixx = confusion_matrix(y_pred, y_true.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(matrixx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "matrix2 = pd.DataFrame(matrixx, index = ['Positive', 'Negative'], columns=['Positive', 'Negative'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "matrix2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.heatmap(matrix2, annot=True, fmt='d')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import precision_recall_fscore_support"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "precision_recall_fscore_support(y_pred, y_true.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(classification_report(y_pred, y_true.values))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import f1_score, precision_score, recall_score\n",
    "\n",
    "# Rappel et précision en fonction de la proba \n",
    "def rappel_precision_f1(prob, ytrue, steps=101):\n",
    "    \"\"\"\n",
    "    Return the recall, precision and f1_score and an array containing each probability for which the metrics where calculated\n",
    "    Steps represents the number of probability taken\n",
    "    \"\"\"\n",
    "    x = np.linspace(0,100,steps)\n",
    "    rap = ()\n",
    "    pre = ()\n",
    "    f1 = ()\n",
    "    for val in x :\n",
    "        ypred=[proba >= val for proba in prob]\n",
    "        rap.append(recall_score(ytrue,ypred))\n",
    "        pre.append(precision_score(ytrue,ypred))\n",
    "        f1.append(f1_score(ytrue,ypred))\n",
    "    return rap, pre, f1, x\n",
    "\n",
    "# Rapport de classification : precision, recall, f1_score, support (sklearn.metrics.precision_recall_fscore_support)\n",
    "\n",
    "\n",
    "# , ou matrice de confusion, (metrics.confusion_matrix)\n",
    "# ou graphique permettant de représenter les mots les plus représentatifs par émotion "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create scatterplot. alpha controls the opacity and s controls the size.\n",
    "ax = sns.scatterplot('anger', extract_most_used_word(df_kaggle['Text'], words=words)[:,0], alpha = 0.5,s = extract_most_used_word(df_kaggle['Text'], words=words)[:,1].astype('int'))\n",
    "\n",
    "# ax.set_xlim(0,6)\n",
    "# ax.set_ylim(-2, 18)\n",
    "\n",
    "#For each point, we add a text inside the bubble\n",
    "for line in range(0,df.shape[0]):\n",
    "     ax.text(df.x[line], df.y[line], df.group[line], horizontalalignment='center', size='medium', color='black', weight='semibold')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Nombre de lignes dupliquées\")\n",
    "print(df_kaggle.duplicated().value_counts())\n",
    "print(\"\\n\" + \"Nombre de valeurs manquantes\")\n",
    "print(df_kaggle.isnull().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
