{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import Pandas\n",
    "import pandas as pd\n",
    "\n",
    "#Set Relative Path\n",
    "import sys\n",
    "sys.path.append(\"../src\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>emotion</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>sadness</td>\n",
       "      <td>Layin n bed with a headache  ughhhh...waitin o...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>sadness</td>\n",
       "      <td>Funeral ceremony...gloomy friday...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>happy</td>\n",
       "      <td>wants to hang out with friends SOON!</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>fear</td>\n",
       "      <td>Re-pinging @ghostridah14: why didn't you go to...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>sadness</td>\n",
       "      <td>I should be sleep, but im not! thinking about ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   emotion                                               text\n",
       "0  sadness  Layin n bed with a headache  ughhhh...waitin o...\n",
       "1  sadness                Funeral ceremony...gloomy friday...\n",
       "2    happy               wants to hang out with friends SOON!\n",
       "3     fear  Re-pinging @ghostridah14: why didn't you go to...\n",
       "4  sadness  I should be sleep, but im not! thinking about ..."
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Import data\n",
    "df_dataK = pd.read_csv(r'../data/d03_cleaned_data/CleanDataworld.csv')\n",
    "df_dataK.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/apprenant/anaconda3/lib/python3.8/site-packages/sklearn/feature_extraction/text.py:383: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['amp', 'aren', 'couldn', 'didn', 'doesn', 'don', 'hadn', 'hasn', 'haven', 'isn', 'let', 'll', 'mustn', 're', 'shan', 'shouldn', 've', 'wasn', 'weren', 'won', 'wouldn'] not in stop_words.\n",
      "  warnings.warn('Your stop_words may be inconsistent with '\n",
      "[4 2 1 5 3 0]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer \n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from wordcloud import STOPWORDS\n",
    "STOPWORDS.update(['feel','feeling','im',\",\",\"t\",\"u\",\"2\",\"'\",\"&amp;\",\"-\",\"...\",\"s\"])\n",
    "\n",
    "# Set X and y\n",
    "tf = TfidfVectorizer(analyzer='word', ngram_range=(1, 2), min_df=0, stop_words=STOPWORDS)\n",
    "tfidf_matrix = tf.fit_transform(df_dataK['text'])\n",
    "X = tfidf_matrix\n",
    "\n",
    "Labelenc = LabelEncoder()\n",
    "df_dataK['emotion'] = Labelenc.fit_transform(df_dataK['emotion'])\n",
    "y = df_dataK['emotion']\n",
    "print (y.unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "i love you so much!!!!\n"
     ]
    }
   ],
   "source": [
    "# Prediction\n",
    "#userinput= str(input()) \n",
    "userinput = \"i love you so much!!!!\"\n",
    "print (userinput)\n",
    "x = tf.transform([userinput])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/apprenant/anaconda3/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:762: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "LogisticRegression()\n",
      "Train R2 = 0.317; Train RMSE = 1.12; Test accuracy = 0.772\n",
      "Test R2  = -0.442; Test RMSE = 1.624; Test accuracy = 0.463\n",
      "[3]\n",
      "['love']\n",
      "\n",
      "\n",
      "SGDClassifier()\n",
      "Train R2 = 0.862; Train RMSE = 0.504; Test accuracy = 0.932\n",
      "Test R2  = -0.379; Test RMSE = 1.583; Test accuracy = 0.466\n",
      "[3]\n",
      "['love']\n",
      "\n",
      "\n",
      "RandomForestClassifier()\n",
      "Train R2 = 0.989; Train RMSE = 0.145; Test accuracy = 0.995\n",
      "Test R2  = -0.448; Test RMSE = 1.629; Test accuracy = 0.451\n",
      "[3]\n",
      "['love']\n",
      "\n",
      "\n",
      "/home/apprenant/anaconda3/lib/python3.8/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n",
      "[10:57:55] WARNING: ../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
      "              colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,\n",
      "              importance_type='gain', interaction_constraints='',\n",
      "              learning_rate=0.300000012, max_delta_step=0, max_depth=6,\n",
      "              min_child_weight=1, missing=nan, monotone_constraints='()',\n",
      "              n_estimators=100, n_jobs=8, num_parallel_tree=1,\n",
      "              objective='multi:softprob', random_state=0, reg_alpha=0,\n",
      "              reg_lambda=1, scale_pos_weight=None, subsample=1,\n",
      "              tree_method='exact', validate_parameters=1, verbosity=None)\n",
      "Train R2 = -0.146; Train RMSE = 1.451; Test accuracy = 0.58\n",
      "Test R2  = -0.468; Test RMSE = 1.638; Test accuracy = 0.454\n",
      "[3]\n",
      "['love']\n",
      "\n",
      "\n",
      "Epoch 1/10\n",
      "120/120 [==============================] - 22s 30ms/step - loss: -247.8442 - accuracy: 0.2805\n",
      "Epoch 2/10\n",
      "120/120 [==============================] - 4s 31ms/step - loss: -32176.5772 - accuracy: 0.2769\n",
      "Epoch 3/10\n",
      "120/120 [==============================] - 4s 34ms/step - loss: -425994.1176 - accuracy: 0.2753\n",
      "Epoch 4/10\n",
      "120/120 [==============================] - 4s 34ms/step - loss: -2119846.9545 - accuracy: 0.2815\n",
      "Epoch 5/10\n",
      "120/120 [==============================] - 4s 31ms/step - loss: -6716363.3058 - accuracy: 0.2744\n",
      "Epoch 6/10\n",
      "120/120 [==============================] - 4s 32ms/step - loss: -15977657.9587 - accuracy: 0.2771\n",
      "Epoch 7/10\n",
      "120/120 [==============================] - 4s 30ms/step - loss: -32148638.0826 - accuracy: 0.2723\n",
      "Epoch 8/10\n",
      "120/120 [==============================] - 4s 30ms/step - loss: -57495203.6694 - accuracy: 0.2775\n",
      "Epoch 9/10\n",
      "120/120 [==============================] - 4s 31ms/step - loss: -96125371.4380 - accuracy: 0.2776\n",
      "Epoch 10/10\n",
      "120/120 [==============================] - 4s 33ms/step - loss: -147426956.2975 - accuracy: 0.2772\n",
      "/home/apprenant/anaconda3/lib/python3.8/site-packages/keras/engine/sequential.py:450: UserWarning: `model.predict_classes()` is deprecated and will be removed after 2021-01-01. Please use instead:* `np.argmax(model.predict(x), axis=-1)`,   if your model does multi-class classification   (e.g. if it uses a `softmax` last-layer activation).* `(model.predict(x) > 0.5).astype(\"int32\")`,   if your model does binary classification   (e.g. if it uses a `sigmoid` last-layer activation).\n",
      "  warnings.warn('`model.predict_classes()` is deprecated and '\n",
      "<keras.engine.sequential.Sequential object at 0x7f2f224dcc70>\n",
      " Train accuracy = 0.276\n",
      " Test accuracy = 0.27\n",
      "[[1.]]\n",
      "['fear']\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from reg_lin import get_metrix\n",
    "from sklearn.linear_model import LogisticRegression ,SGDClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from xgboost import XGBClassifier\n",
    "from keras.models import Sequential \n",
    "from tensorflow.keras import layers, preprocessing\n",
    "from tensorflow import keras\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score,accuracy_score\n",
    "models = [  LogisticRegression(),\n",
    "            SGDClassifier(),\n",
    "            RandomForestClassifier(),\n",
    "            XGBClassifier(),\n",
    "            Sequential()]\n",
    "place = 4\n",
    "savedmodel=[]\n",
    "count = 0\n",
    "for modelset in models :\n",
    "    if count == place:\n",
    "        \n",
    "        df_dataKKeras = df_dataK . sample (15000)\n",
    "        y = df_dataKKeras['emotion']\n",
    "        df_dataKKeras = df_dataKKeras[['text']]\n",
    "        tk = preprocessing.text.Tokenizer()\n",
    "        tk.fit_on_texts(df_dataKKeras['text'])\n",
    "        X = tk.texts_to_matrix(df_dataKKeras['text'], mode='tfidf')\n",
    "        x = tk.texts_to_matrix([userinput], mode='tfidf')\n",
    "    \n",
    "        X_train, X_test, y_train, y_test = train_test_split(X, y, train_size=0.8, random_state=None)\n",
    "        model = modelset\n",
    "        model.add(layers.Dense(160, activation = 'relu', input_shape= [X_train.shape[1]]))\n",
    "        model.add(layers.Dense(80, activation = 'relu'))\n",
    "        model.add(layers.Dense(40, activation='relu'))\n",
    "        model.add(layers.Dense(6,activation='softmax'))\n",
    "        model.compile(optimizer=\"adam\",loss=\"binary_crossentropy\",metrics=['accuracy']) \n",
    "        model.fit(X_train,y_train,epochs=10, batch_size=100)\n",
    "\n",
    "        y_train_pred = model.predict_classes(X_train)\n",
    "        y_test_pred = model.predict_classes(X_test)\n",
    "        \n",
    "        print (model)\n",
    "\n",
    "        print (\" Train accuracy = {}\" \n",
    "        .format(round(accuracy_score(y_train, y_train_pred),3)))\n",
    "\n",
    "        print (\" Test accuracy = {}\" \n",
    "        .format(round(accuracy_score(y_test, y_test_pred),3)))\n",
    "    if count != place:\n",
    "        model = get_metrix(y,X,modelset)\n",
    "    pred = model.predict(x)\n",
    "    print (pred)\n",
    "    if count == place:\n",
    "        pred = Labelenc.inverse_transform([round(pred[0][0])])\n",
    "    if count != place:\n",
    "        pred = Labelenc.inverse_transform(pred)\n",
    "    print (pred)\n",
    "    savedmodel.append(model)\n",
    "    count += 1\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "metadata": {
   "interpreter": {
    "hash": "5b9c502b618e97131917a2f1409b4700bb639cdf99ce16cd88a0e27a90524386"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
