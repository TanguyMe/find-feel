{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import Pandas\n",
    "import pandas as pd\n",
    "\n",
    "#Set Relative Path\n",
    "import sys\n",
    "sys.path.append(\"../src\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>emotion</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>i got up feeling horny this morning</td>\n",
       "      <td>love</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>i will go to my mailbox and talk to the mailma...</td>\n",
       "      <td>happy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>i understand that any of my extremely positive...</td>\n",
       "      <td>happy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>i suggest you give it a listen i feel like i a...</td>\n",
       "      <td>love</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>i have been with petronas for years i feel tha...</td>\n",
       "      <td>happy</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text emotion\n",
       "0                i got up feeling horny this morning    love\n",
       "1  i will go to my mailbox and talk to the mailma...   happy\n",
       "2  i understand that any of my extremely positive...   happy\n",
       "3  i suggest you give it a listen i feel like i a...    love\n",
       "4  i have been with petronas for years i feel tha...   happy"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Import data\n",
    "df_dataK = pd.read_csv(r'../data/d03_cleaned_data/datall.csv')\n",
    "df_dataK.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import WordNetLemmatizer\n",
    "def lemmatizer(text):\n",
    "    \"\"\"Apply Wordnet lemmatizer to text (go to root word)\"\"\"\n",
    "    wnl = WordNetLemmatizer()\n",
    "    text = [wnl.lemmatize(word) for word in text.split()]\n",
    "    return \" \".join(text)\n",
    "df_dataK['text'] = df_dataK['text'].apply(lemmatizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# suppression des caratères spéciaux, formatage des contractions...\n",
    "from nltk.corpus import stopwords\n",
    "from emoji import demojize\n",
    "import re\n",
    "\n",
    "def clean_str(texts):\n",
    "    from nltk.corpus import stopwords\n",
    "    # Lowercasing\n",
    "    texts = texts.str.lower()\n",
    "\n",
    "    # Remove special chars\n",
    "    texts = texts.str.replace(r\"(http|@)\\S+\", \"\")\n",
    "    texts = texts.apply(demojize)\n",
    "    texts = texts.str.replace(r\"::\", \": :\")\n",
    "    texts = texts.str.replace(r\"’\", \"'\")\n",
    "    texts = texts.str.replace(r\"[^a-z\\':_]\", \" \")\n",
    "\n",
    "    # Remove repetitions\n",
    "    pattern = re.compile(r\"(.)\\1{2,}\", re.DOTALL)\n",
    "    texts = texts.str.replace(pattern, r\"\\1\")\n",
    "\n",
    "    # Transform short negation form\n",
    "    texts = texts.str.replace(r\"(can't|cannot)\", 'can not')\n",
    "    texts = texts.str.replace(r\"(ain't|wasn't|weren't)\", 'be not')\n",
    "    texts = texts.str.replace(r\"(don't|didn't|didnt)\", 'do not')\n",
    "    texts = texts.str.replace(r\"(haven't|hasn't)\", 'have not')\n",
    "    texts = texts.str.replace(r\"(won't)\", 'will not')\n",
    "    texts = texts.str.replace(r\"(im)\", ' i am')\n",
    "    texts = texts.str.replace(r\"(ive)\", ' i have')\n",
    "    texts = texts.str.replace(r\"(n't)\", ' not')\n",
    "\n",
    "    # Remove stop words\n",
    "    stopwords = stopwords.words('english')\n",
    "    stopwords.remove('not')\n",
    "    stopwords.remove('nor')\n",
    "    stopwords.remove('no')\n",
    "    texts = texts.apply(lambda x: ' '.join([word for word in x.split() if (word not in stopwords and len(word) > 1 )]))\n",
    "    return texts\n",
    "\n",
    "df_dataK['text'] = clean_str(df_dataK['text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from sentence_transformers import SentenceTransformer\n",
    "def model_used(df, model):\n",
    "    \"\"\"Given a model choice, return the model and the computed matrix\"\"\"\n",
    "    if model == 'Tfidf':\n",
    "        tf = TfidfVectorizer(analyzer='word', ngram_range=(1, 2), min_df=0)\n",
    "        tfidf_matrix = tf.fit_transform(df['text'])\n",
    "        return tf, tfidf_matrix\n",
    "    elif model == 'CountVectorizer':\n",
    "        cv = CountVectorizer(analyzer='word', ngram_range=(1, 2), min_df=0)\n",
    "        matrix = cv.fit_transform(df['text'])\n",
    "        return cv, matrix\n",
    "    elif model == 'BERT':\n",
    "#         bert = SentenceTransformer('distiluse-base-multilingual-cased-v1') # Multilingue\n",
    "#         bert = SentenceTransformer('average_word_embeddings_glove.6B.300d') # + rapide\n",
    "        bert = SentenceTransformer('paraphrase-MiniLM-L6-v2') # Meilleur score en théorie, à vérifier sur nos données\n",
    "        matrix = bert.encode(df['text'].astype('str'), show_progress_bar=True)\n",
    "        return bert, matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You try to use a model that was created with version 1.2.0, however, your version is 1.1.1. This might cause unexpected behavior or errors. In that case, try to update to the latest version.\n",
      "\n",
      "\n",
      "\n",
      "Batches: 100%|██████████| 1489/1489 [03:38<00:00,  6.81it/s]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer \n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "# Set X and y\n",
    "enc, X = model_used(df_dataK,'BERT')\n",
    "\n",
    "Labelenc = LabelEncoder()\n",
    "df_dataK['emotion'] = Labelenc.fit_transform(df_dataK['emotion'])\n",
    "y = df_dataK['emotion']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "i love you so much!!!!\n"
     ]
    }
   ],
   "source": [
    "# Prediction\n",
    "#userinput= str(input()) \n",
    "userinput = \"i love you so much!!!!\"\n",
    "print (userinput)\n",
    "x = enc.encode(clean_str(pd.Series(lemmatizer(userinput))).values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/apprenant/anaconda3/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:762: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "LogisticRegression()\n",
      "Train R2 = -0.292; Train RMSE = 1.589; Test accuracy = 0.532\n",
      "Test R2  = -0.35; Test RMSE = 1.62; Test accuracy = 0.519\n",
      "[3]\n",
      "['love']\n",
      "\n",
      "\n",
      "SGDClassifier()\n",
      "Train R2 = -0.378; Train RMSE = 1.641; Test accuracy = 0.505\n",
      "Test R2  = -0.418; Test RMSE = 1.658; Test accuracy = 0.481\n",
      "[3]\n",
      "['love']\n",
      "\n",
      "\n",
      "RandomForestClassifier()\n",
      "Train R2 = 0.981; Train RMSE = 0.191; Test accuracy = 0.992\n",
      "Test R2  = -0.328; Test RMSE = 1.615; Test accuracy = 0.482\n",
      "[3]\n",
      "['love']\n",
      "\n",
      "\n",
      "/home/apprenant/anaconda3/lib/python3.8/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n",
      "[20:48:33] WARNING: ../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
      "              colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,\n",
      "              importance_type='gain', interaction_constraints='',\n",
      "              learning_rate=0.300000012, max_delta_step=0, max_depth=6,\n",
      "              min_child_weight=1, missing=nan, monotone_constraints='()',\n",
      "              n_estimators=100, n_jobs=8, num_parallel_tree=1,\n",
      "              objective='multi:softprob', random_state=0, reg_alpha=0,\n",
      "              reg_lambda=1, scale_pos_weight=None, subsample=1,\n",
      "              tree_method='exact', validate_parameters=1, verbosity=None)\n",
      "Train R2 = 0.858; Train RMSE = 0.527; Test accuracy = 0.935\n",
      "Test R2  = -0.364; Test RMSE = 1.629; Test accuracy = 0.504\n",
      "[3]\n",
      "['love']\n",
      "\n",
      "\n",
      "Epoch 1/10\n",
      "120/120 [==============================] - 23s 27ms/step - loss: -370.3442 - accuracy: 0.2170\n",
      "Epoch 2/10\n",
      "120/120 [==============================] - 3s 26ms/step - loss: -40654.3385 - accuracy: 0.2227\n",
      "Epoch 3/10\n",
      "120/120 [==============================] - 3s 26ms/step - loss: -480401.7392 - accuracy: 0.2235\n",
      "Epoch 4/10\n",
      "120/120 [==============================] - 3s 26ms/step - loss: -2270366.5888 - accuracy: 0.2226\n",
      "Epoch 5/10\n",
      "120/120 [==============================] - 4s 32ms/step - loss: -6993057.1529 - accuracy: 0.2194\n",
      "Epoch 6/10\n",
      "120/120 [==============================] - 4s 30ms/step - loss: -16879087.1240 - accuracy: 0.2205\n",
      "Epoch 7/10\n",
      "120/120 [==============================] - 4s 30ms/step - loss: -33847434.1157 - accuracy: 0.2183\n",
      "Epoch 8/10\n",
      "120/120 [==============================] - 4s 30ms/step - loss: -59615769.1570 - accuracy: 0.2214\n",
      "Epoch 9/10\n",
      "120/120 [==============================] - 3s 26ms/step - loss: -98199991.3388 - accuracy: 0.2224\n",
      "Epoch 10/10\n",
      "120/120 [==============================] - 3s 24ms/step - loss: -151395425.9174 - accuracy: 0.2222\n",
      "/home/apprenant/anaconda3/lib/python3.8/site-packages/keras/engine/sequential.py:450: UserWarning: `model.predict_classes()` is deprecated and will be removed after 2021-01-01. Please use instead:* `np.argmax(model.predict(x), axis=-1)`,   if your model does multi-class classification   (e.g. if it uses a `softmax` last-layer activation).* `(model.predict(x) > 0.5).astype(\"int32\")`,   if your model does binary classification   (e.g. if it uses a `sigmoid` last-layer activation).\n",
      "  warnings.warn('`model.predict_classes()` is deprecated and '\n",
      "<keras.engine.sequential.Sequential object at 0x7f2876fa1df0>\n",
      " Train accuracy = 0.223\n",
      " Test accuracy = 0.233\n",
      "[[1.]]\n",
      "['fear']\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from reg_lin import get_metrix\n",
    "from sklearn.linear_model import LogisticRegression ,SGDClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from xgboost import XGBClassifier\n",
    "from keras.models import Sequential \n",
    "from tensorflow.keras import layers, preprocessing\n",
    "from tensorflow import keras\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score,accuracy_score\n",
    "models = [  LogisticRegression(n_jobs=-1),\n",
    "            SGDClassifier(n_jobs=-1),\n",
    "            RandomForestClassifier(n_jobs=-1),\n",
    "            XGBClassifier(),\n",
    "            Sequential()]\n",
    "place = 4\n",
    "savedmodel=[]\n",
    "count = 0\n",
    "for modelset in models :\n",
    "    if count == place:\n",
    "        \n",
    "        df_dataKKeras = df_dataK . sample (15000)\n",
    "        y = df_dataKKeras['emotion']\n",
    "\n",
    "        df_dataKKeras = df_dataKKeras[['text']]\n",
    "        tk = preprocessing.text.Tokenizer()\n",
    "        tk.fit_on_texts(df_dataKKeras['text'])\n",
    "        X = tk.texts_to_matrix(df_dataKKeras['text'], mode='tfidf')\n",
    "        \n",
    "        x = tk.texts_to_matrix([userinput], mode='tfidf')\n",
    "    \n",
    "        X_train, X_test, y_train, y_test = train_test_split(X, y, train_size=0.8, random_state=None)\n",
    "        model = modelset\n",
    "        model.add(layers.Dense(160, activation = 'relu', input_shape= [X_train.shape[1]]))\n",
    "        model.add(layers.Dense(80, activation = 'relu'))\n",
    "        model.add(layers.Dense(40, activation='relu'))\n",
    "        model.add(layers.Dense(6,activation='softmax'))\n",
    "        model.compile(optimizer=\"adam\",loss=\"binary_crossentropy\",metrics=['accuracy']) \n",
    "        model.fit(X_train,y_train,epochs=10, batch_size=100)\n",
    "\n",
    "        y_train_pred = model.predict_classes(X_train)\n",
    "        y_test_pred = model.predict_classes(X_test)\n",
    "        \n",
    "        print (model)\n",
    "\n",
    "        print (\" Train accuracy = {}\" \n",
    "        .format(round(accuracy_score(y_train, y_train_pred),3)))\n",
    "\n",
    "        print (\" Test accuracy = {}\" \n",
    "        .format(round(accuracy_score(y_test, y_test_pred),3)))\n",
    "    if count != place:\n",
    "        model = get_metrix(y,X,modelset)\n",
    "    pred = model.predict(x)\n",
    "    print (pred)\n",
    "    if count == place:\n",
    "        pred = Labelenc.inverse_transform([round(pred[0][0])])\n",
    "    if count != place:\n",
    "        pred = Labelenc.inverse_transform(pred)\n",
    "    print (pred)\n",
    "    savedmodel.append(model)\n",
    "    count += 1\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "del x,X,y,X_test,X_train,y_test,y_train,y_test_pred,y_train_pred,df_dataK,df_dataKKeras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'pd' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-5e63fbc6ef29>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Import data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mdf_dataK\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mr'../data/d03_cleaned_data/datall_test.csv'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mdf_dataK\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'text'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf_dataK\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'text'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlemmatizer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mdf_dataK\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'text'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mclean_str\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf_dataK\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'text'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'pd' is not defined"
     ]
    }
   ],
   "source": [
    "# Import data\n",
    "df_dataK = pd.read_csv(r'../data/d03_cleaned_data/datall_test.csv')\n",
    "\n",
    "df_dataK['text'] = df_dataK['text'].apply(lemmatizer)\n",
    "df_dataK['text'] = clean_str(df_dataK['text'])\n",
    "X_test = enc.encode(df_dataK['text'])\n",
    "y_test = Labelenc.transform(df_dataK['emotion'])\n",
    "\n",
    "count = 0\n",
    "place = 4 \n",
    "\n",
    "for model in savedmodel:\n",
    "\n",
    "    print (model)\n",
    "    if count != place:\n",
    "        y_test_pred = model.predict(X_test)\n",
    "    if count == place:\n",
    "        X_test = tk.texts_to_matrix(df_dataK['text'], mode='tfidf')\n",
    "        y_test_pred = model.predict(X_test)\n",
    "    print (\" Test accuracy = {}\" \n",
    "        .format(round(accuracy_score(y_test, y_test_pred),3)))\n",
    "\n",
    "    count += 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "metadata": {
   "interpreter": {
    "hash": "5b9c502b618e97131917a2f1409b4700bb639cdf99ce16cd88a0e27a90524386"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
