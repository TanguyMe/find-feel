{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import Pandas\n",
    "import pandas as pd\n",
    "\n",
    "#Set Relative Path\n",
    "import sys\n",
    "sys.path.append(\"../src\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>emotion</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>i got up feeling horny this morning</td>\n",
       "      <td>love</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>i will go to my mailbox and talk to the mailma...</td>\n",
       "      <td>happy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>i understand that any of my extremely positive...</td>\n",
       "      <td>happy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>i suggest you give it a listen i feel like i a...</td>\n",
       "      <td>love</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>i have been with petronas for years i feel tha...</td>\n",
       "      <td>happy</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text emotion\n",
       "0                i got up feeling horny this morning    love\n",
       "1  i will go to my mailbox and talk to the mailma...   happy\n",
       "2  i understand that any of my extremely positive...   happy\n",
       "3  i suggest you give it a listen i feel like i a...    love\n",
       "4  i have been with petronas for years i feel tha...   happy"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Import data\n",
    "df_dataK = pd.read_csv(r'../data/d03_cleaned_data/datall.csv')\n",
    "df_dataK.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import WordNetLemmatizer\n",
    "def lemmatizer(text):\n",
    "    \"\"\"Apply Wordnet lemmatizer to text (go to root word)\"\"\"\n",
    "    wnl = WordNetLemmatizer()\n",
    "    text = [wnl.lemmatize(word) for word in text.split()]\n",
    "    return \" \".join(text)\n",
    "df_dataK['text'] = df_dataK['text'].apply(lemmatizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# suppression des caratères spéciaux, formatage des contractions...\n",
    "from nltk.corpus import stopwords\n",
    "from emoji import demojize\n",
    "import re\n",
    "\n",
    "def clean_str(texts):\n",
    "    from nltk.corpus import stopwords\n",
    "    # Lowercasing\n",
    "    texts = texts.str.lower()\n",
    "\n",
    "    # Remove special chars\n",
    "    texts = texts.str.replace(r\"(http|@)\\S+\", \"\")\n",
    "    texts = texts.apply(demojize)\n",
    "    texts = texts.str.replace(r\"::\", \": :\")\n",
    "    texts = texts.str.replace(r\"’\", \"'\")\n",
    "    texts = texts.str.replace(r\"[^a-z\\':_]\", \" \")\n",
    "\n",
    "    # Remove repetitions\n",
    "    pattern = re.compile(r\"(.)\\1{2,}\", re.DOTALL)\n",
    "    texts = texts.str.replace(pattern, r\"\\1\")\n",
    "\n",
    "    # Transform short negation form\n",
    "    texts = texts.str.replace(r\"(can't|cannot)\", 'can not')\n",
    "    texts = texts.str.replace(r\"(ain't|wasn't|weren't)\", 'be not')\n",
    "    texts = texts.str.replace(r\"(don't|didn't|didnt)\", 'do not')\n",
    "    texts = texts.str.replace(r\"(haven't|hasn't)\", 'have not')\n",
    "    texts = texts.str.replace(r\"(won't)\", 'will not')\n",
    "    texts = texts.str.replace(r\"(im)\", ' i am')\n",
    "    texts = texts.str.replace(r\"(ive)\", ' i have')\n",
    "    texts = texts.str.replace(r\"(n't)\", ' not')\n",
    "\n",
    "    # Remove stop words\n",
    "    stopwords = stopwords.words('english')\n",
    "    stopwords.remove('not')\n",
    "    stopwords.remove('nor')\n",
    "    stopwords.remove('no')\n",
    "    texts = texts.apply(lambda x: ' '.join([word for word in x.split() if (word not in stopwords and len(word) > 1 )]))\n",
    "    return texts\n",
    "\n",
    "df_dataK['text'] = clean_str(df_dataK['text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from sentence_transformers import SentenceTransformer\n",
    "def model_used(df, model):\n",
    "    \"\"\"Given a model choice, return the model and the computed matrix\"\"\"\n",
    "    if model == 'Tfidf':\n",
    "        tf = TfidfVectorizer(analyzer='word', ngram_range=(1, 2), min_df=0)\n",
    "        tfidf_matrix = tf.fit_transform(df['text'])\n",
    "        return tf, tfidf_matrix\n",
    "    elif model == 'CountVectorizer':\n",
    "        cv = CountVectorizer(analyzer='word', ngram_range=(1, 2), min_df=0)\n",
    "        matrix = cv.fit_transform(df['text'])\n",
    "        return cv, matrix\n",
    "    elif model == 'BERT':\n",
    "#         bert = SentenceTransformer('distiluse-base-multilingual-cased-v1') # Multilingue\n",
    "#         bert = SentenceTransformer('average_word_embeddings_glove.6B.300d') # + rapide\n",
    "        bert = SentenceTransformer('paraphrase-MiniLM-L6-v2') # Meilleur score en théorie, à vérifier sur nos données\n",
    "        matrix = bert.encode(df['text'].astype('str'), show_progress_bar=True)\n",
    "        return bert, matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You try to use a model that was created with version 1.2.0, however, your version is 1.1.1. This might cause unexpected behavior or errors. In that case, try to update to the latest version.\n",
      "\n",
      "\n",
      "\n",
      "Batches: 100%|██████████| 1489/1489 [03:22<00:00,  7.36it/s]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer \n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "# Set X and y\n",
    "enc, X = model_used(df_dataK,'BERT')\n",
    "\n",
    "Labelenc = LabelEncoder()\n",
    "df_dataK['emotion'] = Labelenc.fit_transform(df_dataK['emotion'])\n",
    "y = df_dataK['emotion']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "i love you so much!!!!\n"
     ]
    }
   ],
   "source": [
    "# Prediction\n",
    "#userinput= str(input()) \n",
    "userinput = \"i love you so much!!!!\"\n",
    "print (userinput)\n",
    "x = enc.encode(clean_str(pd.Series(lemmatizer(userinput))).values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 10 candidates, totalling 30 fits\n",
      "[CV] n_estimators=10, min_samples_split=4, min_samples_leaf=16, criterion=entropy \n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[CV]  n_estimators=10, min_samples_split=4, min_samples_leaf=16, criterion=entropy, score=0.441, total=   5.0s\n",
      "[CV] n_estimators=10, min_samples_split=4, min_samples_leaf=16, criterion=entropy \n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    5.0s remaining:    0.0s\n",
      "[CV]  n_estimators=10, min_samples_split=4, min_samples_leaf=16, criterion=entropy, score=0.438, total=   3.6s\n",
      "[CV] n_estimators=10, min_samples_split=4, min_samples_leaf=16, criterion=entropy \n",
      "[Parallel(n_jobs=1)]: Done   2 out of   2 | elapsed:    8.6s remaining:    0.0s\n",
      "[CV]  n_estimators=10, min_samples_split=4, min_samples_leaf=16, criterion=entropy, score=0.439, total=   3.4s\n",
      "[CV] n_estimators=100, min_samples_split=2, min_samples_leaf=8, criterion=gini \n",
      "[CV]  n_estimators=100, min_samples_split=2, min_samples_leaf=8, criterion=gini, score=0.479, total=  13.4s\n",
      "[CV] n_estimators=100, min_samples_split=2, min_samples_leaf=8, criterion=gini \n",
      "[CV]  n_estimators=100, min_samples_split=2, min_samples_leaf=8, criterion=gini, score=0.467, total=  12.9s\n",
      "[CV] n_estimators=100, min_samples_split=2, min_samples_leaf=8, criterion=gini \n",
      "[CV]  n_estimators=100, min_samples_split=2, min_samples_leaf=8, criterion=gini, score=0.467, total=  13.0s\n",
      "[CV] n_estimators=500, min_samples_split=16, min_samples_leaf=4, criterion=entropy \n",
      "[CV]  n_estimators=500, min_samples_split=16, min_samples_leaf=4, criterion=entropy, score=0.486, total= 2.1min\n",
      "[CV] n_estimators=500, min_samples_split=16, min_samples_leaf=4, criterion=entropy \n",
      "[CV]  n_estimators=500, min_samples_split=16, min_samples_leaf=4, criterion=entropy, score=0.476, total= 2.3min\n",
      "[CV] n_estimators=500, min_samples_split=16, min_samples_leaf=4, criterion=entropy \n",
      "[CV]  n_estimators=500, min_samples_split=16, min_samples_leaf=4, criterion=entropy, score=0.474, total= 2.5min\n",
      "[CV] n_estimators=10, min_samples_split=4, min_samples_leaf=2, criterion=entropy \n",
      "[CV]  n_estimators=10, min_samples_split=4, min_samples_leaf=2, criterion=entropy, score=0.409, total=   4.1s\n",
      "[CV] n_estimators=10, min_samples_split=4, min_samples_leaf=2, criterion=entropy \n",
      "[CV]  n_estimators=10, min_samples_split=4, min_samples_leaf=2, criterion=entropy, score=0.409, total=   4.2s\n",
      "[CV] n_estimators=10, min_samples_split=4, min_samples_leaf=2, criterion=entropy \n",
      "[CV]  n_estimators=10, min_samples_split=4, min_samples_leaf=2, criterion=entropy, score=0.406, total=   4.5s\n",
      "[CV] n_estimators=50, min_samples_split=4, min_samples_leaf=8, criterion=gini \n",
      "[CV]  n_estimators=50, min_samples_split=4, min_samples_leaf=8, criterion=gini, score=0.475, total=   7.3s\n",
      "[CV] n_estimators=50, min_samples_split=4, min_samples_leaf=8, criterion=gini \n",
      "[CV]  n_estimators=50, min_samples_split=4, min_samples_leaf=8, criterion=gini, score=0.467, total=   7.4s\n",
      "[CV] n_estimators=50, min_samples_split=4, min_samples_leaf=8, criterion=gini \n",
      "[CV]  n_estimators=50, min_samples_split=4, min_samples_leaf=8, criterion=gini, score=0.466, total=   8.2s\n",
      "[CV] n_estimators=100, min_samples_split=16, min_samples_leaf=4, criterion=gini \n",
      "[CV]  n_estimators=100, min_samples_split=16, min_samples_leaf=4, criterion=gini, score=0.477, total=  15.5s\n",
      "[CV] n_estimators=100, min_samples_split=16, min_samples_leaf=4, criterion=gini \n",
      "[CV]  n_estimators=100, min_samples_split=16, min_samples_leaf=4, criterion=gini, score=0.473, total=  16.2s\n",
      "[CV] n_estimators=100, min_samples_split=16, min_samples_leaf=4, criterion=gini \n",
      "[CV]  n_estimators=100, min_samples_split=16, min_samples_leaf=4, criterion=gini, score=0.472, total=  16.2s\n",
      "[CV] n_estimators=500, min_samples_split=16, min_samples_leaf=2, criterion=gini \n",
      "[CV]  n_estimators=500, min_samples_split=16, min_samples_leaf=2, criterion=gini, score=0.486, total= 1.4min\n",
      "[CV] n_estimators=500, min_samples_split=16, min_samples_leaf=2, criterion=gini \n",
      "[CV]  n_estimators=500, min_samples_split=16, min_samples_leaf=2, criterion=gini, score=0.478, total= 1.4min\n",
      "[CV] n_estimators=500, min_samples_split=16, min_samples_leaf=2, criterion=gini \n",
      "[CV]  n_estimators=500, min_samples_split=16, min_samples_leaf=2, criterion=gini, score=0.474, total= 1.4min\n",
      "[CV] n_estimators=200, min_samples_split=16, min_samples_leaf=8, criterion=entropy \n",
      "[CV]  n_estimators=200, min_samples_split=16, min_samples_leaf=8, criterion=entropy, score=0.476, total= 1.0min\n",
      "[CV] n_estimators=200, min_samples_split=16, min_samples_leaf=8, criterion=entropy \n",
      "[CV]  n_estimators=200, min_samples_split=16, min_samples_leaf=8, criterion=entropy, score=0.472, total= 1.1min\n",
      "[CV] n_estimators=200, min_samples_split=16, min_samples_leaf=8, criterion=entropy \n",
      "[CV]  n_estimators=200, min_samples_split=16, min_samples_leaf=8, criterion=entropy, score=0.470, total=  59.7s\n",
      "[CV] n_estimators=500, min_samples_split=4, min_samples_leaf=2, criterion=entropy \n",
      "[CV]  n_estimators=500, min_samples_split=4, min_samples_leaf=2, criterion=entropy, score=0.489, total= 2.8min\n",
      "[CV] n_estimators=500, min_samples_split=4, min_samples_leaf=2, criterion=entropy \n",
      "[CV]  n_estimators=500, min_samples_split=4, min_samples_leaf=2, criterion=entropy, score=0.478, total= 2.7min\n",
      "[CV] n_estimators=500, min_samples_split=4, min_samples_leaf=2, criterion=entropy \n",
      "[CV]  n_estimators=500, min_samples_split=4, min_samples_leaf=2, criterion=entropy, score=0.477, total= 2.4min\n",
      "[CV] n_estimators=1000, min_samples_split=8, min_samples_leaf=2, criterion=entropy \n",
      "[CV]  n_estimators=1000, min_samples_split=8, min_samples_leaf=2, criterion=entropy, score=0.489, total= 4.9min\n",
      "[CV] n_estimators=1000, min_samples_split=8, min_samples_leaf=2, criterion=entropy \n",
      "[CV]  n_estimators=1000, min_samples_split=8, min_samples_leaf=2, criterion=entropy, score=0.477, total= 5.0min\n",
      "[CV] n_estimators=1000, min_samples_split=8, min_samples_leaf=2, criterion=entropy \n",
      "[CV]  n_estimators=1000, min_samples_split=8, min_samples_leaf=2, criterion=entropy, score=0.479, total= 4.9min\n",
      "[Parallel(n_jobs=1)]: Done  30 out of  30 | elapsed: 39.2min finished\n",
      "RandomizedSearchCV(cv=3, estimator=RandomForestClassifier(n_jobs=-1),\n",
      "                   param_distributions={'criterion': ['gini', 'entropy'],\n",
      "                                        'min_samples_leaf': [2, 4, 8, 16],\n",
      "                                        'min_samples_split': [2, 4, 8, 16],\n",
      "                                        'n_estimators': [10, 20, 50, 100, 200,\n",
      "                                                         500, 1000]},\n",
      "                   scoring='accuracy', verbose=3)\n",
      "Train R2 = 0.979; Train RMSE = 0.203; Test accuracy = 0.992\n",
      "Test R2  = -0.318; Test RMSE = 1.604; Test accuracy = 0.485\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'n_estimators': 1000,\n",
       " 'min_samples_split': 8,\n",
       " 'min_samples_leaf': 2,\n",
       " 'criterion': 'entropy'}"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from reg_lin import get_metrix\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "params_LogisticRegression={\n",
    "    \"criterion\":    ['gini', 'entropy'],    \n",
    "    \"n_estimators\":[10,20,50,100,200,500,1000],\n",
    "    \"min_samples_split\":[2,4,8,16],\n",
    "    \"min_samples_leaf\":[2,4,8,16]}\n",
    "    \n",
    "model = RandomForestClassifier(n_jobs=-1)\n",
    "truc = RandomizedSearchCV(model, param_distributions=params_LogisticRegression, scoring=\"accuracy\",cv=3,verbose=3)\n",
    "\n",
    "get_metrix(y,X,truc)\n",
    "\n",
    "truc.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.48179933338582265"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "truc.best_score_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "closing parenthesis ')' does not match opening parenthesis '{' on line 12 (<ipython-input-2-f47ce9092c25>, line 14)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-2-f47ce9092c25>\"\u001b[0;36m, line \u001b[0;32m14\u001b[0m\n\u001b[0;31m    )\u001b[0m\n\u001b[0m    ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m closing parenthesis ')' does not match opening parenthesis '{' on line 12\n"
     ]
    }
   ],
   "source": [
    "from reg_lin import get_metrix\n",
    "from sklearn.linear_model import LogisticRegression ,SGDClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from xgboost import XGBClassifier\n",
    "from keras.models import Sequential \n",
    "from tensorflow.keras import layers, preprocessing\n",
    "from tensorflow import keras\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score,accuracy_score\n",
    "\n",
    "\n",
    "models = [  LogisticRegression(n_jobs=-1,multi_class='multinomial',penalty='l2',solver = 'saga' , max_iter=500,c=1 ),\n",
    "            SGDClassifier(n_jobs=-1,penalty='l2', max_iter=500,loss='hinge',alpha=0.001),\n",
    "            RandomForestClassifier(n_jobs=-1,n_estimators= 1000,min_samples_split= 8,min_samples_leaf= 2,criterion= 'entropy'),\n",
    "            XGBClassifier(),\n",
    "            Sequential())\n",
    "\n",
    "place = 4\n",
    "savedmodel=[]\n",
    "count = 0\n",
    "for modelset in models :\n",
    "    if count == place:\n",
    "        \n",
    "        df_dataKKeras = df_dataK . sample (15000)\n",
    "        y = df_dataKKeras['emotion']\n",
    "\n",
    "        df_dataKKeras = df_dataKKeras[['text']]\n",
    "        tk = preprocessing.text.Tokenizer()\n",
    "        tk.fit_on_texts(df_dataKKeras['text'])\n",
    "        X = tk.texts_to_matrix(df_dataKKeras['text'], mode='tfidf')\n",
    "        \n",
    "        x = tk.texts_to_matrix([userinput], mode='tfidf')\n",
    "    \n",
    "        X_train, X_test, y_train, y_test = train_test_split(X, y, train_size=0.8, random_state=None)\n",
    "        model = modelset\n",
    "        model.add(layers.Dense(160, activation = 'relu', input_shape= [X_train.shape[1]]))\n",
    "        model.add(layers.Dense(80, activation = 'relu'))\n",
    "        model.add(layers.Dense(40, activation='relu'))\n",
    "        model.add(layers.Dense(6,activation='softmax'))\n",
    "        model.compile(optimizer=\"adam\",loss=\"binary_crossentropy\",metrics=['accuracy']) \n",
    "        model.fit(X_train,y_train,epochs=10, batch_size=100)\n",
    "\n",
    "        y_train_pred = model.predict_classes(X_train)\n",
    "        y_test_pred = model.predict_classes(X_test)\n",
    "        \n",
    "        print (model)\n",
    "\n",
    "        print (\" Train accuracy = {}\" \n",
    "        .format(round(accuracy_score(y_train, y_train_pred),3)))\n",
    "\n",
    "        print (\" Test accuracy = {}\" \n",
    "        .format(round(accuracy_score(y_test, y_test_pred),3)))\n",
    "    if count != place:\n",
    "        model = get_metrix(y,X,modelset)\n",
    "    pred = model.predict(x)\n",
    "    print (pred)\n",
    "    if count == place:\n",
    "        pred = Labelenc.inverse_transform([round(pred[0][0])])\n",
    "    if count != place:\n",
    "        pred = Labelenc.inverse_transform(pred)\n",
    "    print (pred)\n",
    "    savedmodel.append(model)\n",
    "    count += 1\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "del x,X,y,X_test,X_train,y_test,y_train,y_test_pred,y_train_pred,df_dataK,df_dataKKeras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'pd' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-5e63fbc6ef29>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Import data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mdf_dataK\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mr'../data/d03_cleaned_data/datall_test.csv'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mdf_dataK\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'text'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf_dataK\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'text'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlemmatizer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mdf_dataK\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'text'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mclean_str\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf_dataK\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'text'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'pd' is not defined"
     ]
    }
   ],
   "source": [
    "# Import data\n",
    "df_dataK = pd.read_csv(r'../data/d03_cleaned_data/datall_test.csv')\n",
    "\n",
    "df_dataK['text'] = df_dataK['text'].apply(lemmatizer)\n",
    "df_dataK['text'] = clean_str(df_dataK['text'])\n",
    "X_test = enc.encode(df_dataK['text'])\n",
    "y_test = Labelenc.transform(df_dataK['emotion'])\n",
    "\n",
    "count = 0\n",
    "place = 4 \n",
    "\n",
    "for model in savedmodel:\n",
    "\n",
    "    print (model)\n",
    "    if count != place:\n",
    "        y_test_pred = model.predict(X_test)\n",
    "    if count == place:\n",
    "        X_test = tk.texts_to_matrix(df_dataK['text'], mode='tfidf')\n",
    "        y_test_pred = model.predict(X_test)\n",
    "    print (\" Test accuracy = {}\" \n",
    "        .format(round(accuracy_score(y_test, y_test_pred),3)))\n",
    "\n",
    "    count += 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "metadata": {
   "interpreter": {
    "hash": "5b9c502b618e97131917a2f1409b4700bb639cdf99ce16cd88a0e27a90524386"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
